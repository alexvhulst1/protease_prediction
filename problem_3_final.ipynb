{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ijson\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "051724\n",
      "Data extracted from PhosphoSitePlus(R), created by Cell Signaling Technology Inc. PhosphoSitePlus is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. Attribution must be given in written, oral and digital presentations to PhosphoSitePlus, www.phosphosite.org. Written documents should additionally cite Hornbeck PV, Kornhauser JM, Tkachev S, Zhang B, Skrzypek E, Murray B, Latham V, Sullivan M (2012) PhosphoSitePlus: a comprehensive resource for investigating the structure and function of experimentally determined post-translational modifications in man and mouse. Nucleic Acids Res. 40, D261Ã70.; www.phosphosite.org.\n",
      "\n",
      "GENE\tKINASE\tKIN_ACC_ID\tKIN_ORGANISM\tSUBSTRATE\tSUB_GENE_ID\tSUB_ACC_ID\tSUB_GENE\tSUB_ORGANISM\tSUB_MOD_RSD\tSITE_GRP_ID\tSITE_+/-7_AA\tDOMAIN\tIN_VIVO_RXN\tIN_VITRO_RXN\tCST_CAT#\n",
      "Dyrk2\tDYRK2\tQ5U4C9\tmouse\tNDEL1\t83431\tQ9ERR1\tNdel1\tmouse\tS336\t1869686801\tLGSsRPSsAPGMLPL\t\t \tX\t\n"
     ]
    }
   ],
   "source": [
    "#data recovery\n",
    "\n",
    "path = \"Kinase_Substrate_Dataset\"\n",
    "\n",
    "with open(path, encoding=\"latin-1\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23470\n",
      "21612\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data with complete information\n",
    "\n",
    "count = 0\n",
    "\n",
    "kin = []\n",
    "sub = []\n",
    "site = []\n",
    "\n",
    "with open(path, encoding=\"latin-1\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 3:\n",
    "            mots = line.split()\n",
    "            mots_de_longueur_6 = [mot for mot in mots if len(mot) == 6]\n",
    "            #mot de longueur 6 qui contienne une lettre puis 5 chiffres\n",
    "            mots_de_longueur_6_lettre_chiffre = [mot for mot in mots_de_longueur_6 if mot[0].isalpha() and mot[1].isdigit()]\n",
    "            if len(mots_de_longueur_6_lettre_chiffre) == 2:\n",
    "                kin_acc = mots_de_longueur_6_lettre_chiffre[0]\n",
    "                sub_acc = mots_de_longueur_6_lettre_chiffre[1]\n",
    "                id_sub_acc = mots.index(sub_acc)\n",
    "                try:\n",
    "                    site_ = mots[id_sub_acc + 3]\n",
    "                    try:\n",
    "                        site_ = int(site_[1:])\n",
    "                        kin.append(kin_acc)\n",
    "                        sub.append(sub_acc)\n",
    "                        site.append(site_)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            count += 1\n",
    "\n",
    "print(count)\n",
    "print(len(kin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_kinase:  803\n",
      "length of data after removing incomplete line:  21612\n",
      "example of list_kinase:  ['Q9BXM7', 'Q63644', 'P11799', 'P47197', 'Q923T9']\n"
     ]
    }
   ],
   "source": [
    "list_kinase = []\n",
    "\n",
    "for i in range(len(kin)):\n",
    "    list_kinase.append(kin[i])\n",
    "        \n",
    "list_kinase = list(set(list_kinase))\n",
    "print(\"length of list_kinase: \", len(list_kinase))\n",
    "print(\"length of data after removing incomplete line: \", len(kin))\n",
    "print(\"example of list_kinase: \", list_kinase[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_substrates:  4977\n",
      "length of data after removing incomplete line:  21612\n",
      "example of list_substrates:  ['Q9WTU3', 'P15336', 'B0LPN4', 'Q969T9', 'O35245']\n"
     ]
    }
   ],
   "source": [
    "list_substrates = list(set(sub))\n",
    "print(\"length of list_substrates: \", len(list_substrates))\n",
    "print(\"length of data after removing incomplete line: \", len(sub))\n",
    "print(\"example of list_substrates: \", list_substrates[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 570820\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each substrate (embedding here)\n",
    "\n",
    "dic_sub = {}\n",
    "\n",
    "for i in range(len(list_substrates)):\n",
    "    dic_sub[list_substrates[i]] = []\n",
    "\n",
    "i = 0\n",
    "with h5py.File(\"../per-protein.h5\", \"r\") as file:\n",
    "    print(f\"number of entries: {len(file.items())}\")\n",
    "    for sequence_id, embedding in file.items():\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(i)\n",
    "        if sequence_id in dic_sub:\n",
    "            dic_sub[sequence_id].append(np.array(embedding).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each substrate (GO and sequence here)\n",
    "\n",
    "i = 0\n",
    "with open('../uniprotkb_AND_reviewed_true_2024_03_26.json', \"rb\") as f:\n",
    "    for record in ijson.items(f, \"results.item\"):\n",
    "        try:\n",
    "            i += 1\n",
    "            refs = record.get(\"uniProtKBCrossReferences\", [])\n",
    "            if record[\"primaryAccession\"] in dic_sub:\n",
    "                GO = [ref[\"id\"] for ref in refs if ref.get(\"database\") == \"GO\"]\n",
    "                sequence = record[\"sequence\"][\"value\"]\n",
    "                dic_sub[record[\"primaryAccession\"]] = [dic_sub[record[\"primaryAccession\"]], GO, sequence]\n",
    "                    \n",
    "            if i % 50000 == 0:\n",
    "                print(i)\n",
    "                \n",
    "        except Exception as record_error:\n",
    "            print(\"Error processing record:\", record_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of each output to keep the kinase for which we have enough data\n",
    "\n",
    "count_kinase_dic = {}\n",
    "\n",
    "for enzyme in list_kinase:\n",
    "    count_kinase_dic[enzyme] = 0\n",
    "\n",
    "for i in range(len(kin)):\n",
    "    count_kinase_dic[kin[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc = []    #substrate\n",
    "X_seq = []    #sequence\n",
    "X_site = []   #site\n",
    "X_GO = []     #GO\n",
    "X_emb = []    #embedding\n",
    "Y = []        #kinase\n",
    "\n",
    "for i in range(len(sub)):\n",
    "    if len(dic_sub[sub[i]]) == 3 and len(dic_sub[sub[i]][0]) > 0 and count_kinase_dic[kin[i]] >= 5: #we keep only the E3 for which we have enough data\n",
    "        X_acc.append(sub[i])\n",
    "        X_seq.append(dic_sub[sub[i]][2])\n",
    "        X_site.append(site[i])\n",
    "        X_GO.append(dic_sub[sub[i]][1])\n",
    "        X_emb.append(dic_sub[sub[i]][0][0])\n",
    "        Y.append(kin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20638\n",
      "P34901\n",
      "MAPVCLFAPLLLLLLGGFPVAPGESIRETEVIDPQDLLEGRYFSGALPDDEDAGGLEQDSDFELSGSGDLDDTEEPRTFPEVISPLVPLDNHIPENAQPGIRVPSEPKELEENEVIPKRVPSDVGDDDVSNKVSMSSTSQGSNIFERTEVLAALIVGGVVGILFAVFLILLLVYRMKKKDEGSYDLGKKPIYKKAPTNEFYA\n",
      "183\n",
      "['GO:0009986', 'GO:0043034', 'GO:0005576', 'GO:0005925', 'GO:0005886', 'GO:0001968', 'GO:0042802', 'GO:0005080', 'GO:0070053', 'GO:0007155', 'GO:0016477', 'GO:0007267', 'GO:0060122', 'GO:0042130', 'GO:0001843', 'GO:1903543', 'GO:1903553', 'GO:0051894', 'GO:0051496', 'GO:0010762', 'GO:0001657', 'GO:0042060']\n",
      "[0.050628662109375, -0.043853759765625, 0.06488037109375, 0.019989013671875, 0.00875091552734375, 0.073486328125, -0.03387451171875, -0.039215087890625, 0.0170745849609375, 0.04632568359375, 0.00888824462890625, -0.031402587890625, 0.01019287109375, -0.0276947021484375, 0.0203399658203125, 0.0518798828125, 0.0135040283203125, 0.01422119140625, -0.0292816162109375, -0.0277099609375, -0.0294189453125, -0.0189361572265625, -0.045654296875, -0.0225372314453125, -0.0150604248046875, 0.030609130859375, 0.04815673828125, -0.04248046875, -0.01190948486328125, -0.0335693359375, 0.0156707763671875, 0.148193359375, -0.1402587890625, -0.059295654296875, -0.1229248046875, 0.00516510009765625, -0.03021240234375, 0.0289764404296875, -0.031402587890625, 0.0312042236328125, 0.00787353515625, 0.01338958740234375, -0.05126953125, -0.005649566650390625, -0.01506805419921875, -0.0160369873046875, 0.038818359375, 0.02508544921875, 0.05596923828125, 0.047515869140625, 0.024383544921875, 0.07928466796875, -0.0333251953125, -0.0263214111328125, 0.007617950439453125, 0.0010728836059570312, -0.01377105712890625, -0.0740966796875, 0.00865936279296875, -0.006504058837890625, -0.01415252685546875, -0.006591796875, 0.00540924072265625, 0.00550079345703125, -0.01654052734375, 0.0160369873046875, -0.09564208984375, -0.04205322265625, -0.00054931640625, -0.032196044921875, -0.047454833984375, 0.0106048583984375, -0.1180419921875, 0.00357818603515625, 0.01165008544921875, 0.02838134765625, -0.03704833984375, 0.005435943603515625, -0.03118896484375, 0.0247039794921875, 0.023101806640625, 0.064697265625, 0.0164947509765625, 0.0452880859375, -0.034393310546875, 0.0256195068359375, -0.0095672607421875, -0.03656005859375, 0.03216552734375, 0.039306640625, 0.070556640625, -0.03131103515625, -0.046417236328125, -0.0496826171875, 0.0263519287109375, -0.028167724609375, -0.0909423828125, 0.07470703125, -0.046661376953125, -0.0399169921875, -0.0021305084228515625, 0.038177490234375, 0.0146636962890625, 0.11236572265625, 0.038726806640625, -0.026123046875, -0.021484375, -0.0009260177612304688, 0.024932861328125, -0.08154296875, 0.028411865234375, 0.0194244384765625, -0.057464599609375, 0.002704620361328125, 0.06103515625, 0.0286712646484375, 0.0259857177734375, -0.08905029296875, 0.0010976791381835938, -0.01983642578125, -0.052642822265625, 0.01256561279296875, 0.02728271484375, 0.01071929931640625, -0.034332275390625, -0.00957489013671875, -0.0274505615234375, 0.027191162109375, 0.031402587890625, 0.0166778564453125, 0.043365478515625, 0.031982421875, -0.0218658447265625, -0.060028076171875, -0.0543212890625, -0.0304718017578125, 0.0175323486328125, 0.056549072265625, -0.0164337158203125, -0.03228759765625, 0.06121826171875, 0.01143646240234375, -0.0755615234375, -0.02398681640625, -0.183349609375, 0.004039764404296875, -0.03204345703125, -0.00467681884765625, 0.0176849365234375, -0.0301055908203125, -0.020965576171875, 0.00630950927734375, 0.0222625732421875, -0.01161956787109375, 0.020721435546875, 0.0247344970703125, 0.106689453125, 0.0060882568359375, 0.002422332763671875, -0.01158905029296875, 0.052978515625, -0.060394287109375, -0.034149169921875, -0.01898193359375, -0.0413818359375, -0.0230712890625, 0.0921630859375, 0.04541015625, 0.001514434814453125, -0.00885009765625, 0.0158233642578125, 0.0070648193359375, -0.005664825439453125, 0.1082763671875, 0.057525634765625, -0.038116455078125, -0.00948333740234375, 0.0390625, -0.06268310546875, 0.03204345703125, 0.08831787109375, 0.055419921875, -0.03680419921875, -0.0092315673828125, 0.0017757415771484375, -0.041412353515625, -0.00832366943359375, 0.0406494140625, 0.031768798828125, -0.022918701171875, 0.1434326171875, 0.045074462890625, 0.012542724609375, 0.015716552734375, 0.077392578125, 0.10906982421875, -0.0204010009765625, 0.0254669189453125, 0.04144287109375, -0.039886474609375, -0.030853271484375, 0.029693603515625, -0.01995849609375, -0.0875244140625, 0.0517578125, -0.0914306640625, 0.1680908203125, -0.048614501953125, 0.045867919921875, -0.0511474609375, -0.01021575927734375, -0.042144775390625, -0.0265350341796875, -0.0014562606811523438, 0.032135009765625, 0.05059814453125, 0.034942626953125, -0.05426025390625, -0.031890869140625, -0.036590576171875, -0.043914794921875, -0.035491943359375, -0.0021610260009765625, 0.0347900390625, 0.0099945068359375, 0.0997314453125, -0.06390380859375, -0.02587890625, -0.0151519775390625, 0.07623291015625, -0.056640625, 0.0704345703125, -0.04071044921875, 0.0220489501953125, -0.032806396484375, -0.0008807182312011719, -0.03759765625, 0.039764404296875, -0.0419921875, 0.02117919921875, -0.0030918121337890625, -0.018096923828125, -0.057373046875, -0.0386962890625, -0.0190887451171875, 0.023223876953125, -0.07965087890625, 0.005718231201171875, 0.03729248046875, -0.025177001953125, 0.0301666259765625, -0.06610107421875, -0.0748291015625, -0.07073974609375, 0.049041748046875, 0.01324462890625, 0.020111083984375, 0.02069091796875, 0.020751953125, -0.002117156982421875, -0.050872802734375, -0.03173828125, -0.043975830078125, 0.055999755859375, 0.033843994140625, 0.03814697265625, 0.0298614501953125, -0.10540771484375, -0.0293731689453125, -0.003414154052734375, 0.01806640625, -0.01434326171875, -0.035797119140625, -0.1248779296875, 0.0225982666015625, -0.036468505859375, 0.0213165283203125, -0.059661865234375, -0.0251617431640625, -0.126708984375, -0.0102691650390625, 0.051544189453125, 0.022674560546875, 0.00907135009765625, 0.028045654296875, -0.062744140625, 0.031219482421875, 0.01502227783203125, -0.06329345703125, 0.03582763671875, 0.05999755859375, 0.059051513671875, -0.0587158203125, -0.0015916824340820312, -0.0421142578125, 0.0007452964782714844, -0.016357421875, -0.0606689453125, 0.01605224609375, 0.0275115966796875, 0.034698486328125, 0.0423583984375, -0.04705810546875, -0.052642822265625, 0.08331298828125, 0.0303497314453125, -0.007099151611328125, -0.0787353515625, 0.014251708984375, 0.0928955078125, -0.0162200927734375, 0.02685546875, -0.006103515625, -0.038299560546875, -0.028076171875, 0.0168914794921875, 0.0325927734375, 0.01560211181640625, 0.053497314453125, 0.07818603515625, -0.005645751953125, -0.03375244140625, 0.00045871734619140625, 0.01479339599609375, 0.01233673095703125, 0.0260009765625, -0.02716064453125, -0.0171051025390625, 0.0396728515625, -0.04718017578125, 0.01464080810546875, -0.046478271484375, -0.018707275390625, -0.0231475830078125, 0.03668212890625, 0.07672119140625, 0.047576904296875, 0.06854248046875, 0.007450103759765625, -0.003253936767578125, -0.0032215118408203125, 0.0174407958984375, 0.036407470703125, 0.00487518310546875, 0.0019025802612304688, -0.025360107421875, -0.004199981689453125, -0.003841400146484375, 0.0304412841796875, 0.044403076171875, -0.01137542724609375, 0.041534423828125, -0.0128021240234375, -0.060882568359375, 0.0192108154296875, -0.0296783447265625, -0.056182861328125, -0.033843994140625, -0.025146484375, 0.053466796875, -0.0296630859375, -0.01213836669921875, -0.04296875, 0.041748046875, 0.03424072265625, 0.053009033203125, -0.1080322265625, -0.0921630859375, 0.0242767333984375, 0.039825439453125, -0.0282745361328125, -0.0288543701171875, -0.0235137939453125, 0.004825592041015625, -0.06829833984375, 0.01025390625, -0.021240234375, -0.04547119140625, 0.082275390625, -0.038604736328125, -0.033416748046875, 0.0731201171875, 0.0027828216552734375, -0.0211029052734375, -0.0107879638671875, -0.03790283203125, -0.0194091796875, 0.00554656982421875, -0.0279998779296875, -0.023712158203125, 0.0023021697998046875, 0.048797607421875, 0.06683349609375, -0.0604248046875, 0.04913330078125, -0.0303192138671875, 0.05218505859375, -0.0003795623779296875, 0.0270233154296875, 0.0438232421875, -0.0030155181884765625, 0.00995635986328125, 0.0256500244140625, 0.016326904296875, -0.034454345703125, -0.11474609375, 0.033111572265625, 0.0968017578125, -0.026641845703125, -0.139404296875, -0.051849365234375, 0.015899658203125, 0.039276123046875, -0.014617919921875, 0.001857757568359375, -0.0034961700439453125, -0.0259552001953125, 0.060516357421875, -0.002574920654296875, -0.038818359375, 0.052337646484375, -0.02386474609375, 0.05987548828125, 0.048248291015625, 0.0282440185546875, 0.01065826416015625, 0.0135650634765625, 0.06011962890625, -0.0299835205078125, 0.0257415771484375, -0.0716552734375, -0.094970703125, -0.01528167724609375, 0.03204345703125, 0.0186614990234375, 0.01090240478515625, -0.07366943359375, 0.1041259765625, -0.06689453125, -0.06353759765625, 0.03314208984375, 0.026641845703125, -0.025604248046875, 0.1192626953125, -0.045684814453125, 0.033233642578125, -0.0160980224609375, 0.0634765625, 0.04864501953125, 0.017486572265625, 0.05487060546875, -0.002254486083984375, 0.00957489013671875, -0.03692626953125, 0.0185546875, 0.03448486328125, -0.0174560546875, 0.015899658203125, -0.0123748779296875, 0.029083251953125, 0.0123748779296875, -0.09429931640625, 0.035430908203125, 0.00910186767578125, 0.0555419921875, -0.0791015625, 0.08038330078125, 0.01500701904296875, 0.04290771484375, -0.05181884765625, 0.0911865234375, -0.023193359375, 0.0296478271484375, -0.04852294921875, 0.0178985595703125, 0.0247802734375, -0.020965576171875, 0.037811279296875, -0.0232391357421875, -0.019805908203125, 0.10174560546875, -0.031951904296875, -0.03167724609375, 0.0010433197021484375, -0.005615234375, 0.08197021484375, -0.03497314453125, 0.013763427734375, 0.005313873291015625, -0.045806884765625, 0.025360107421875, 0.0906982421875, -0.0211334228515625, 0.014190673828125, 0.010894775390625, -0.0416259765625, -0.0478515625, -0.01617431640625, -0.017547607421875, 0.024749755859375, -0.0203399658203125, 0.04046630859375, -0.01910400390625, -0.006252288818359375, -0.031494140625, -0.0035915374755859375, -0.007274627685546875, 0.0703125, -0.0216217041015625, 0.01654052734375, 0.016815185546875, 0.02655029296875, -0.03228759765625, 0.020904541015625, -0.04766845703125, -0.046966552734375, 0.0084686279296875, 0.0227508544921875, -0.0005669593811035156, 0.03839111328125, -0.01554107666015625, 0.0211181640625, -0.00930023193359375, 0.029998779296875, -0.08502197265625, -0.014739990234375, 0.043701171875, 0.009368896484375, 0.0007319450378417969, -0.06597900390625, -0.05828857421875, -0.035003662109375, -0.0015840530395507812, 0.005268096923828125, 0.042205810546875, -0.03973388671875, -0.01299285888671875, -0.07232666015625, 0.0080108642578125, 0.045074462890625, -0.03582763671875, 0.014434814453125, 0.02191162109375, 0.0209197998046875, -0.06121826171875, 0.067626953125, 0.02496337890625, -0.037261962890625, -0.04736328125, -0.0026416778564453125, 0.0167236328125, -0.08953857421875, 0.0018033981323242188, 0.0946044921875, -0.005252838134765625, -0.009002685546875, -0.032806396484375, 0.027923583984375, 0.0231475830078125, -0.005126953125, -0.01160430908203125, 0.0176849365234375, -0.007328033447265625, -0.058441162109375, 0.0333251953125, 0.0382080078125, -0.032867431640625, -0.039154052734375, 0.0006189346313476562, -0.017181396484375, -0.0214080810546875, -0.010284423828125, -0.047454833984375, -0.003337860107421875, -0.039642333984375, 0.0017099380493164062, -0.0333251953125, 0.002964019775390625, 0.08953857421875, 0.0190582275390625, 0.0238189697265625, -0.01531982421875, 0.00897216796875, 0.0548095703125, -0.0018949508666992188, -0.0545654296875, -0.0011510848999023438, -0.005718231201171875, -0.00742340087890625, 0.035980224609375, 0.0218505859375, -0.04345703125, 0.096435546875, -0.0012044906616210938, -0.026641845703125, 0.054534912109375, -0.018157958984375, -0.00318145751953125, 0.01092529296875, 0.017974853515625, -0.0236358642578125, -0.01052093505859375, -0.048431396484375, 0.040771484375, -0.0212554931640625, 0.04766845703125, 0.0295257568359375, -0.0870361328125, 0.0450439453125, -0.056396484375, -0.00524139404296875, -0.027069091796875, 0.00647735595703125, -0.006805419921875, -0.01253509521484375, 0.159912109375, -0.039459228515625, -0.0029354095458984375, 0.103271484375, 0.021240234375, -0.0021800994873046875, -0.018707275390625, -0.0518798828125, 0.025390625, -0.0118560791015625, -0.042724609375, -0.023529052734375, 0.0262908935546875, -0.058929443359375, 0.05084228515625, 0.08648681640625, 0.10223388671875, 0.046600341796875, -0.03790283203125, -0.053497314453125, 0.01477813720703125, 0.052642822265625, -0.0278472900390625, -0.01000213623046875, 0.0219879150390625, 0.01120758056640625, -0.0286102294921875, -0.01245880126953125, 0.0092010498046875, 0.0599365234375, 0.0682373046875, -0.053863525390625, 0.0255889892578125, -0.0380859375, 0.0260467529296875, 0.034423828125, 0.0174102783203125, -0.031341552734375, -0.041259765625, -0.0112152099609375, 0.0142822265625, 0.040008544921875, -0.043792724609375, 0.00646209716796875, -0.057342529296875, -0.01430511474609375, -0.018035888671875, 0.005863189697265625, -0.018768310546875, 0.016632080078125, 0.0016002655029296875, -0.03179931640625, 0.00791168212890625, 0.01068115234375, 0.0146942138671875, 0.08441162109375, 0.04541015625, -0.04095458984375, -0.0682373046875, 0.01708984375, -0.039306640625, 0.0224456787109375, -0.029937744140625, 0.0506591796875, 0.03179931640625, 0.026641845703125, 0.04095458984375, -0.00406646728515625, 0.161376953125, 0.041717529296875, -0.052459716796875, 0.0364990234375, -0.0164031982421875, -0.0284881591796875, -0.014404296875, 0.0297088623046875, -0.058319091796875, 0.0029888153076171875, -0.06396484375, 0.005168914794921875, -0.08203125, -0.00763702392578125, -0.00937652587890625, -0.0845947265625, -0.050262451171875, 0.0189208984375, 0.070068359375, -0.0193328857421875, 0.051788330078125, 0.02239990234375, 0.020050048828125, -0.00782012939453125, -0.033233642578125, 0.0298919677734375, 0.039215087890625, 0.03216552734375, -0.00018835067749023438, 0.0841064453125, -0.01105499267578125, -0.033294677734375, -0.005706787109375, 0.03887939453125, -0.00975799560546875, -0.024261474609375, 0.061126708984375, -0.0050506591796875, -0.041290283203125, 0.0006394386291503906, -0.040985107421875, 0.050445556640625, -0.054290771484375, -0.011871337890625, 0.0252685546875, -0.0026493072509765625, -0.04730224609375, -0.0002155303955078125, 0.0218505859375, -0.042510986328125, 0.0335693359375, -0.02838134765625, 0.01267242431640625, 0.01207733154296875, 0.053558349609375, -0.0242767333984375, 0.170166015625, -0.035186767578125, 0.02056884765625, -0.060821533203125, -0.0312347412109375, -0.050018310546875, -0.040740966796875, -0.01082611083984375, 0.05999755859375, 0.08050537109375, 0.06585693359375, -0.027679443359375, 0.08599853515625, -0.038330078125, 0.0273284912109375, 0.0479736328125, -0.0254058837890625, -0.02056884765625, 0.043548583984375, 0.01552581787109375, 0.009124755859375, -0.056884765625, -0.0257568359375, -0.0008821487426757812, -0.053680419921875, -0.0087890625, -0.0517578125, 0.04193115234375, 0.0176239013671875, 0.059356689453125, 0.00881195068359375, 0.0469970703125, 0.032958984375, 0.0245513916015625, 0.014556884765625, -0.0182952880859375, 0.0296173095703125, -0.0379638671875, 0.01244354248046875, 0.034149169921875, 0.0914306640625, 0.0382080078125, 0.0780029296875, 0.005313873291015625, 0.004791259765625, 0.053466796875, -0.020263671875, 0.0028285980224609375, -0.10565185546875, 0.0043792724609375, 0.0226287841796875, 0.0389404296875, 0.0195159912109375, -0.019561767578125, -0.001590728759765625, -0.00907135009765625, 0.06646728515625, -0.05902099609375, 0.00849151611328125, -0.046966552734375, 0.0062255859375, 0.0105743408203125, -0.048431396484375, -0.0009860992431640625, 0.0275726318359375, -0.00963592529296875, -0.052276611328125, 0.0234832763671875, -0.052032470703125, 0.036651611328125, 0.03509521484375, 0.067138671875, -0.0271148681640625, -0.00402069091796875, -0.01177978515625, 0.1358642578125, -0.0014257431030273438, 0.016387939453125, -0.0265655517578125, -0.0235595703125, -0.066650390625, 0.0015430450439453125, -0.001312255859375, -0.00548553466796875, 0.0526123046875, -0.00037407875061035156, 0.044342041015625, 0.0266876220703125, -0.0023479461669921875, -0.004596710205078125, -0.0701904296875, -0.039276123046875, -0.0535888671875, 0.01049041748046875, -0.06103515625, 0.037841796875, -0.024322509765625, 0.000881195068359375, 0.01471710205078125, 0.0550537109375, -0.0379638671875, 0.03448486328125, -0.0643310546875, 0.043182373046875, -0.032073974609375, -0.07427978515625, 0.0171661376953125, -0.03515625, -0.004375457763671875, -0.025665283203125, 0.10284423828125, 0.031341552734375, 0.0084075927734375, 0.01094818115234375, -0.0305938720703125, -0.0267486572265625, 0.033660888671875, -0.03851318359375, -0.064453125, 0.041015625, 0.0155029296875, 0.026611328125, 0.0138702392578125, 0.0261993408203125, 0.01186370849609375, 0.053314208984375, 0.036529541015625, -0.032318115234375, 0.10125732421875, -0.035675048828125, -0.0947265625, 0.0261688232421875, 0.0180206298828125, 0.08831787109375, 0.087646484375, 0.05517578125, -0.042999267578125, -0.07049560546875, -0.0078887939453125, 0.026641845703125, -0.051849365234375, 0.07086181640625, -0.02349853515625, -0.057281494140625, 0.0025806427001953125, -0.0362548828125, -0.07086181640625, 0.0823974609375, -0.017791748046875, 0.034912109375, -0.005046844482421875, -0.00443267822265625, 0.034912109375, 0.05157470703125, -0.0157012939453125, 0.033782958984375, 0.037872314453125, 0.049652099609375, 0.00572967529296875, 0.11572265625, -0.06719970703125, -0.11309814453125, -0.057464599609375, -0.0849609375, 0.02197265625, 0.010009765625, 0.0538330078125, -0.031402587890625, 0.0214996337890625, 0.044342041015625, -0.0201568603515625, 0.01202392578125, -0.08367919921875, -0.09716796875, 0.0146026611328125, -0.018829345703125, 0.060302734375, -0.09356689453125, 0.01375579833984375, -0.0221099853515625, -0.036041259765625, -0.017333984375, 0.0185394287109375, 0.0299530029296875, -0.11602783203125, -0.039581298828125, 0.043975830078125, 0.0249481201171875, 0.0256500244140625, 0.04779052734375, 0.0146636962890625, 0.0142974853515625, -0.0313720703125, 0.005817413330078125, 0.02789306640625, -0.01739501953125, 0.042205810546875, 0.0032215118408203125, -0.032958984375, 0.01416778564453125, 0.030242919921875, 0.0049285888671875, 0.037109375, -0.017547607421875, -0.0025959014892578125, -0.020416259765625, -0.0687255859375, -0.0360107421875, -0.081298828125, 0.037322998046875, 0.01390838623046875, -0.001628875732421875, -0.0213470458984375, 0.0101165771484375, -0.102783203125, -0.055938720703125, -0.0171661376953125, -0.0198974609375, 0.0335693359375, -0.0204010009765625, -0.00982666015625, 0.01107025146484375, 0.005252838134765625, 0.05267333984375, 0.0148468017578125, -0.0145416259765625, -0.06695556640625, -0.00225830078125, 0.0413818359375, 0.0114288330078125, 0.005817413330078125, 0.04052734375, 0.0760498046875, -0.048614501953125, -0.045379638671875, -0.07208251953125, -0.00537872314453125, -0.0242919921875, -0.0269927978515625, 0.03466796875, 0.0037784576416015625, 0.007801055908203125, -0.0016908645629882812, -0.0101318359375, -0.0406494140625, -0.007778167724609375, 0.046112060546875, -0.0985107421875, 0.003814697265625, -0.018035888671875, 0.0028667449951171875, 0.07037353515625, 0.01141357421875, 0.0209808349609375, -0.05096435546875, 0.03594970703125, -0.09747314453125, -0.028656005859375, 0.02069091796875, 0.048065185546875, 0.0038127899169921875, 0.08770751953125, 0.006198883056640625, -0.0253143310546875, 0.01519012451171875, 0.1304931640625, -0.061279296875, -0.01311492919921875, 0.0165557861328125, -0.035064697265625, 0.06365966796875, -0.003223419189453125, -0.00743865966796875, -0.034271240234375, -0.04034423828125, -0.06329345703125, 0.0100250244140625, -0.04376220703125, -0.01386260986328125, 0.0204620361328125, 0.0279541015625, -0.0033626556396484375, -0.0302581787109375, -0.023712158203125, 0.0264892578125]\n",
      "Q05655\n"
     ]
    }
   ],
   "source": [
    "print(len(X_acc))\n",
    "print(X_acc[0])\n",
    "print(X_seq[0])\n",
    "print(X_site[0])\n",
    "print(X_GO[0])\n",
    "print(X_emb[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of go terms:  12744\n",
      "example:  [('GO:0009986', 1217), ('GO:0043034', 56), ('GO:0005576', 943), ('GO:0005925', 1644), ('GO:0005886', 7479)]\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "#encoding of the go terms\n",
    "\n",
    "count_go = {}\n",
    "for i in range(len(X_GO)):\n",
    "    for go in X_GO[i]:\n",
    "        if go in count_go:\n",
    "            count_go[go] += 1\n",
    "        else:\n",
    "            count_go[go] = 1\n",
    "\n",
    "print(\"number of go terms: \", len(count_go))\n",
    "print(\"example: \", list(count_go.items())[0:5])\n",
    "\n",
    "#we keep the most frequent go terms\n",
    "number_go = 2000\n",
    "\n",
    "most_frequent_go = dict(sorted(count_go.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "most_frequent_go = dict(list(most_frequent_go.items())[0:number_go])\n",
    "\n",
    "X_GO_filtered = []\n",
    "\n",
    "for i in range(len(X_GO)):\n",
    "    go_filtered = []\n",
    "    for go in X_GO[i]:\n",
    "        if go in most_frequent_go:\n",
    "            go_filtered.append(go)\n",
    "    X_GO_filtered.append(go_filtered)\n",
    "\n",
    "# encoder X_GO_filtered\n",
    "\n",
    "list_go = list(most_frequent_go.keys())\n",
    "dic_GO = {}\n",
    "for i in range(len(list_go)):\n",
    "    dic_GO[list_go[i]] = i\n",
    "\n",
    "X_GO_filtered_int = []\n",
    "\n",
    "for i in range(len(X_GO_filtered)):\n",
    "    go = X_GO_filtered[i]\n",
    "    go_int = [0]*len(list_go)\n",
    "    for j in range(len(go)):\n",
    "        go_int[dic_GO[go[j]]] = 1\n",
    "    X_GO_filtered_int.append(go_int)\n",
    "\n",
    "print(X_GO_filtered_int[0])\n",
    "print(len(X_GO_filtered_int[0]))\n",
    "\n",
    "#enregistrement of dic_GO\n",
    "\n",
    "with open('dic_GO_problem_3.json', 'w') as fp:\n",
    "    json.dump(dic_GO, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding of the cleavage environement\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "X_pep = []\n",
    "for i in range(len(X_seq)):\n",
    "    seq = X_seq[i]\n",
    "    p1 = X_site[i]\n",
    "    pep = []\n",
    "    min = p1-window_size\n",
    "    max = p1+window_size\n",
    "    for j in range(min,max):\n",
    "        if j < 0:\n",
    "            pep.append('X')\n",
    "        elif j >= len(seq):\n",
    "            pep.append('X')\n",
    "        else:\n",
    "            pep.append(seq[j])\n",
    "    X_pep.append(pep)\n",
    "\n",
    "#tranformation of peptides into vocab\n",
    "vocab = ['A','B','C','D','E','F','G','H','I','K','L','M','N','0','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "vocab_dict = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_dict[vocab[i]] = i\n",
    "\n",
    "X_pep_int = []\n",
    "for i in range(len(X_pep)):\n",
    "    pep = X_pep[i]\n",
    "    pep_int = []\n",
    "    for j in range(len(pep)):\n",
    "        pep_int.append(vocab_dict[pep[j]])\n",
    "    X_pep_int.append(pep_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding of kin names\n",
    "list_kinase = list(set(Y))\n",
    "dic_kinase = {}\n",
    "for i in range(len(list_kinase)):\n",
    "    dic_kinase[list_kinase[i]] = i\n",
    "\n",
    "y_int = []\n",
    "for i in range(len(Y)):\n",
    "    y_encoded = [0]*len(list_kinase)\n",
    "    y_encoded[dic_kinase[Y[i]]] = 1\n",
    "    y_int.append(y_encoded)\n",
    "\n",
    "#enregistrement of dic_kinase\n",
    "\n",
    "with open('dic_kinase.json', 'w') as fp:\n",
    "    json.dump(dic_kinase, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20638\n",
      "20638\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "455\n"
     ]
    }
   ],
   "source": [
    "print(len(X_pep_int))\n",
    "print(len(y_int))\n",
    "print(y_int[0])\n",
    "print(len(y_int[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_embedding_t = torch.tensor(X_emb, dtype=torch.float32).to(device)\n",
    "X_pep_t = torch.tensor(X_pep_int, dtype=torch.int64).to(device)\n",
    "X_site_t = torch.tensor(X_site, dtype=torch.float32).to(device)\n",
    "X_site_t = X_site_t.unsqueeze(1).to(device)\n",
    "X_go_t = torch.tensor(X_GO_filtered_int, dtype=torch.float32).to(device)\n",
    "y_t = torch.tensor(y_int, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_embedding_pep(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim):\n",
    "        super(model_embedding_pep, self).__init__()\n",
    "        self.fc_embed = nn.Linear(embed_dim, 2048)\n",
    "        self.embed = nn.Embedding(26, 128)\n",
    "        self.gru = nn.GRU(128, 256, 2, batch_first=True)\n",
    "        self.fc_pep = nn.Linear(256, 2048)\n",
    "        self.fc1 = nn.Linear(2048 * 2, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "    \n",
    "    def forward(self, embed, pep, site):\n",
    "        x_embed = F.relu(self.fc_embed(embed))\n",
    "        x_pep = self.embed(pep)\n",
    "        x_pep, _ = self.gru(x_pep)\n",
    "        x_pep = x_pep[:,-1,:]\n",
    "        x_pep = F.relu(self.fc_pep(x_pep))\n",
    "        x = torch.cat((x_embed, x_pep), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class model_embedding_pep_go(nn.Module):\n",
    "    def __init__(self, embed_dim, go_dim, output_dim):\n",
    "        super(model_embedding_pep_go, self).__init__()\n",
    "        self.fc_embed = nn.Linear(embed_dim, 2048)\n",
    "        self.embed = nn.Embedding(26, 128)\n",
    "        self.gru = nn.GRU(128, 256, 2, batch_first=True)\n",
    "        self.fc_pep = nn.Linear(256, 2048)\n",
    "        self.fc_go = nn.Linear(go_dim, 2048)\n",
    "        self.fc1 = nn.Linear(2048 * 3, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "    \n",
    "    def forward(self, embed, pep, site, go):\n",
    "        x_embed = F.relu(self.fc_embed(embed))\n",
    "        x_pep = self.embed(pep)\n",
    "        x_pep, _ = self.gru(x_pep)\n",
    "        x_pep = x_pep[:,-1,:]\n",
    "        x_pep = F.relu(self.fc_pep(x_pep))\n",
    "        x_go = F.relu(self.fc_go(go))\n",
    "        x = torch.cat((x_embed, x_pep, x_go), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, pep, site, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed, pep, site, go)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_with_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, pep, site, go, labels = data\n",
    "            outputs = model(embed, pep, site, go)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] in predicted3[i]:\n",
    "                    correct3 += 1\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train loss: 0.013 | Test loss: 0.012 | Test accuracy: 0.141 | Test top 3 accuracy: 0.287\n",
      "Epoch 2/100 | Train loss: 0.011 | Test loss: 0.010 | Test accuracy: 0.201 | Test top 3 accuracy: 0.374\n",
      "Epoch 3/100 | Train loss: 0.009 | Test loss: 0.010 | Test accuracy: 0.223 | Test top 3 accuracy: 0.411\n",
      "Epoch 4/100 | Train loss: 0.008 | Test loss: 0.009 | Test accuracy: 0.254 | Test top 3 accuracy: 0.452\n",
      "Epoch 5/100 | Train loss: 0.008 | Test loss: 0.009 | Test accuracy: 0.264 | Test top 3 accuracy: 0.482\n",
      "Epoch 6/100 | Train loss: 0.007 | Test loss: 0.009 | Test accuracy: 0.284 | Test top 3 accuracy: 0.507\n",
      "Epoch 7/100 | Train loss: 0.006 | Test loss: 0.009 | Test accuracy: 0.300 | Test top 3 accuracy: 0.517\n",
      "Epoch 8/100 | Train loss: 0.006 | Test loss: 0.009 | Test accuracy: 0.300 | Test top 3 accuracy: 0.514\n",
      "Epoch 9/100 | Train loss: 0.006 | Test loss: 0.009 | Test accuracy: 0.302 | Test top 3 accuracy: 0.522\n",
      "Epoch 10/100 | Train loss: 0.005 | Test loss: 0.009 | Test accuracy: 0.300 | Test top 3 accuracy: 0.530\n",
      "Epoch 11/100 | Train loss: 0.005 | Test loss: 0.009 | Test accuracy: 0.298 | Test top 3 accuracy: 0.539\n",
      "Epoch 12/100 | Train loss: 0.005 | Test loss: 0.009 | Test accuracy: 0.301 | Test top 3 accuracy: 0.537\n",
      "Epoch 13/100 | Train loss: 0.004 | Test loss: 0.009 | Test accuracy: 0.302 | Test top 3 accuracy: 0.538\n",
      "Epoch 14/100 | Train loss: 0.004 | Test loss: 0.009 | Test accuracy: 0.298 | Test top 3 accuracy: 0.537\n",
      "Epoch 15/100 | Train loss: 0.004 | Test loss: 0.010 | Test accuracy: 0.301 | Test top 3 accuracy: 0.545\n",
      "Epoch 16/100 | Train loss: 0.004 | Test loss: 0.010 | Test accuracy: 0.295 | Test top 3 accuracy: 0.541\n",
      "Epoch 17/100 | Train loss: 0.004 | Test loss: 0.010 | Test accuracy: 0.298 | Test top 3 accuracy: 0.555\n",
      "Epoch 18/100 | Train loss: 0.004 | Test loss: 0.010 | Test accuracy: 0.295 | Test top 3 accuracy: 0.545\n",
      "Epoch 19/100 | Train loss: 0.003 | Test loss: 0.010 | Test accuracy: 0.293 | Test top 3 accuracy: 0.543\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_pep_train, X_pep_test, X_site_train, X_site_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_pep_t, X_site_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_pep_train, X_site_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_pep_test, X_site_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of kinases = 455\n",
    "\n",
    "model = model_embedding_pep_go(embed_dim, go_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_with_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_with_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_pep_go_site_problem_3_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, pep, site, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed, pep, site)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_without_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, pep, site, go, labels = data\n",
    "            outputs = model(embed, pep, site)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] in predicted3[i]:\n",
    "                    correct3 += 1\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train loss: 0.013 | Test loss: 0.012 | Test accuracy: 0.111 | Test top 3 accuracy: 0.242\n",
      "Epoch 2/100 | Train loss: 0.012 | Test loss: 0.011 | Test accuracy: 0.160 | Test top 3 accuracy: 0.308\n",
      "Epoch 3/100 | Train loss: 0.011 | Test loss: 0.011 | Test accuracy: 0.166 | Test top 3 accuracy: 0.328\n",
      "Epoch 4/100 | Train loss: 0.010 | Test loss: 0.011 | Test accuracy: 0.180 | Test top 3 accuracy: 0.344\n",
      "Epoch 5/100 | Train loss: 0.010 | Test loss: 0.010 | Test accuracy: 0.197 | Test top 3 accuracy: 0.371\n",
      "Epoch 6/100 | Train loss: 0.009 | Test loss: 0.010 | Test accuracy: 0.206 | Test top 3 accuracy: 0.381\n",
      "Epoch 7/100 | Train loss: 0.009 | Test loss: 0.010 | Test accuracy: 0.211 | Test top 3 accuracy: 0.393\n",
      "Epoch 8/100 | Train loss: 0.009 | Test loss: 0.010 | Test accuracy: 0.210 | Test top 3 accuracy: 0.399\n",
      "Epoch 9/100 | Train loss: 0.008 | Test loss: 0.010 | Test accuracy: 0.222 | Test top 3 accuracy: 0.412\n",
      "Epoch 10/100 | Train loss: 0.008 | Test loss: 0.010 | Test accuracy: 0.231 | Test top 3 accuracy: 0.417\n",
      "Epoch 11/100 | Train loss: 0.007 | Test loss: 0.010 | Test accuracy: 0.223 | Test top 3 accuracy: 0.418\n",
      "Epoch 12/100 | Train loss: 0.007 | Test loss: 0.010 | Test accuracy: 0.219 | Test top 3 accuracy: 0.429\n",
      "Epoch 13/100 | Train loss: 0.007 | Test loss: 0.010 | Test accuracy: 0.224 | Test top 3 accuracy: 0.420\n",
      "Epoch 14/100 | Train loss: 0.006 | Test loss: 0.010 | Test accuracy: 0.233 | Test top 3 accuracy: 0.426\n",
      "Epoch 15/100 | Train loss: 0.006 | Test loss: 0.010 | Test accuracy: 0.222 | Test top 3 accuracy: 0.415\n",
      "Epoch 16/100 | Train loss: 0.006 | Test loss: 0.011 | Test accuracy: 0.221 | Test top 3 accuracy: 0.421\n",
      "Epoch 17/100 | Train loss: 0.006 | Test loss: 0.011 | Test accuracy: 0.221 | Test top 3 accuracy: 0.425\n",
      "Epoch 18/100 | Train loss: 0.005 | Test loss: 0.011 | Test accuracy: 0.224 | Test top 3 accuracy: 0.422\n",
      "Epoch 19/100 | Train loss: 0.005 | Test loss: 0.011 | Test accuracy: 0.219 | Test top 3 accuracy: 0.422\n",
      "Epoch 20/100 | Train loss: 0.005 | Test loss: 0.011 | Test accuracy: 0.218 | Test top 3 accuracy: 0.420\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_pep_train, X_pep_test, X_site_train, X_site_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_pep_t, X_site_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_pep_train, X_site_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_pep_test, X_site_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of kinase = 455\n",
    "\n",
    "model = model_embedding_pep(embed_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_without_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_without_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_pep_site_problem_3_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
