{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ijson\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179079\n"
     ]
    }
   ],
   "source": [
    "#data recovery\n",
    "\n",
    "data = pd.read_csv('cleavage.csv')\n",
    "data = data.drop(0)\n",
    "data.head()\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86675\n",
      "code                                                     A01.001\n",
      "uniprot_acc                                               P00004\n",
      "p1                                                            11\n",
      "cleavage_evidence                                   experimental\n",
      "cleavage_type                                  non-physiological\n",
      "cleavage_notes                                               NaN\n",
      "cleavage_ontology                                            NaN\n",
      "cleavage_location                                            NaN\n",
      "residue_range                                              2-105\n",
      "cutdb                                                        NaN\n",
      "Ref                         <%Hamuro %etal, 2008[20080325A077]%>\n",
      "peptide_identification                                        MS\n",
      "peptidase_identification                                     NaN\n",
      "mernum                                                       NaN\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = []\n",
    "\n",
    "#suppression of data without precise output\n",
    "for i in range(len(data)):\n",
    "    if data.iloc[i,0][0:3] != \"CLE\":\n",
    "        data_cleaned.append(data.iloc[i])\n",
    "\n",
    "print(len(data_cleaned))\n",
    "print(data_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_enzymes:  1229\n",
      "length of data after removing incomplete line:  86675\n",
      "example of list_enzymes:  ['M41.013', 'C32.003', 'S9G.094', 'N09.001', 'M12.311']\n"
     ]
    }
   ],
   "source": [
    "list_enzymes = []\n",
    "for i in range(1, len(data_cleaned)):\n",
    "    list_enzymes.append(data_cleaned[i][\"code\"])\n",
    "        \n",
    "list_enzymes = list(set(list_enzymes))\n",
    "print(\"length of list_enzymes: \", len(list_enzymes))\n",
    "print(\"length of data after removing incomplete line: \", len(data_cleaned))\n",
    "print(\"example of list_enzymes: \", list_enzymes[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_proteins:  14274\n",
      "length of data after removing incomplete line:  86675\n",
      "example of list_proteins:  ['Q9WTU0', 'Q5BKY9', 'Q13586', 'P08218', 'Q43715']\n"
     ]
    }
   ],
   "source": [
    "list_proteins = []\n",
    "for i in range(1, len(data_cleaned)):\n",
    "    list_proteins.append(data_cleaned[i][\"uniprot_acc\"])\n",
    "        \n",
    "list_proteins = list(set(list_proteins))\n",
    "print(\"length of list_proteins: \", len(list_proteins))\n",
    "print(\"length of data after removing incomplete line: \", len(data_cleaned))\n",
    "print(\"example of list_proteins: \", list_proteins[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each protein (GO and sequence here)\n",
    "\n",
    "dic_protein = {}\n",
    "for protein in list_proteins:\n",
    "    dic_protein[protein] = {}\n",
    "\n",
    "i = 0\n",
    "with open('../uniprotkb_AND_reviewed_true_2024_03_26.json', \"rb\") as f:\n",
    "    for record in ijson.items(f, \"results.item\"):\n",
    "        try:\n",
    "            i += 1\n",
    "            refs = record.get(\"uniProtKBCrossReferences\", [])\n",
    "            if record[\"primaryAccession\"] in list_proteins:\n",
    "                GO = [ref[\"id\"] for ref in refs if ref.get(\"database\") == \"GO\"]\n",
    "                sequence = record[\"sequence\"][\"value\"]\n",
    "                dic_protein[record[\"primaryAccession\"]] = [GO, sequence]\n",
    "                    \n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "                \n",
    "        except Exception as record_error:\n",
    "            print(\"Error processing record:\", record_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 570820\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each protein (embedding here)\n",
    "\n",
    "i = 0\n",
    "with h5py.File(\"../per-protein.h5\", \"r\") as file:\n",
    "    print(f\"number of entries: {len(file.items())}\")\n",
    "    for sequence_id, embedding in file.items():\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        if sequence_id in dic_protein:\n",
    "            dic_protein[sequence_id].append(np.array(embedding).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of each output to keep the enzyme for which we have enough data\n",
    "\n",
    "count_enzyme_dic = {}\n",
    "\n",
    "for enzyme in list_enzymes:\n",
    "    count_enzyme_dic[enzyme] = 0\n",
    "\n",
    "for i in range(0, len(data_cleaned)):\n",
    "    count_enzyme_dic[data_cleaned[i][\"code\"]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cl = []   #cleavage_site\n",
    "y_name = []   #name of enzyme\n",
    "y_index = []   #index of the data\n",
    "X_seq = []   #sequence of protein\n",
    "X_embedding = []   #embedding of protein\n",
    "X_GO = []   #GO of protein\n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(0, len(data_cleaned)):\n",
    "    enzyme = data_cleaned[i][\"code\"]\n",
    "    protein = data_cleaned[i][\"uniprot_acc\"]\n",
    "    if len(dic_protein[protein]) == 3 and count_enzyme_dic[enzyme] >= 5:    #all information about protein and more than 5 data for the enzyme\n",
    "        sequence = dic_protein[protein][1]\n",
    "        prot_embedding = dic_protein[protein][2]\n",
    "        cleavage = int(data_cleaned[i][\"p1\"])\n",
    "        go = dic_protein[protein][0]\n",
    "        X_cl.append(cleavage)\n",
    "        y_name.append(enzyme)\n",
    "        y_index.append(j)\n",
    "        X_seq.append(sequence)\n",
    "        X_embedding.append(prot_embedding)\n",
    "        X_GO.append(go)\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of go terms:  15411\n",
      "example:  [('GO:0070069', 28), ('GO:0005829', 42159), ('GO:0005758', 1025), ('GO:0070469', 675), ('GO:0009055', 1275)]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "#encoding of the go terms\n",
    "\n",
    "count_go = {}\n",
    "\n",
    "for i in range(len(X_GO)):\n",
    "    for go in X_GO[i]:\n",
    "        if go in count_go:\n",
    "            count_go[go] += 1\n",
    "        else:\n",
    "            count_go[go] = 1\n",
    "\n",
    "print(\"number of go terms: \", len(count_go))\n",
    "print(\"example: \", list(count_go.items())[0:5])\n",
    "\n",
    "#we keep the most frequent go terms\n",
    "number_go = 2000\n",
    "\n",
    "most_frequent_go = dict(sorted(count_go.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "most_frequent_go = dict(list(most_frequent_go.items())[0:number_go])\n",
    "\n",
    "X_GO_filtered = []\n",
    "\n",
    "for i in range(len(X_GO)):\n",
    "    go_filtered = []\n",
    "    for go in X_GO[i]:\n",
    "        if go in most_frequent_go:\n",
    "            go_filtered.append(go)\n",
    "    X_GO_filtered.append(go_filtered)\n",
    "\n",
    "# encoder X_GO_filtered\n",
    "\n",
    "list_go = list(most_frequent_go.keys())\n",
    "dic_GO = {}\n",
    "for i in range(len(list_go)):\n",
    "    dic_GO[list_go[i]] = i\n",
    "\n",
    "X_GO_filtered_int = []\n",
    "\n",
    "for i in range(len(X_GO_filtered)):\n",
    "    go = X_GO_filtered[i]\n",
    "    go_int = [0]*len(list_go)\n",
    "    for j in range(len(go)):\n",
    "        go_int[dic_GO[go[j]]] = 1\n",
    "    X_GO_filtered_int.append(go_int)\n",
    "\n",
    "print(X_GO_filtered_int[0])\n",
    "print(len(X_GO_filtered_int[0]))\n",
    "\n",
    "#enregistrement of dic_GO\n",
    "\n",
    "with open('dic_GO_problem_1.json', 'w') as fp:\n",
    "    json.dump(dic_GO, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding of the cleavage environement\n",
    "\n",
    "window_size = 10\n",
    "\n",
    "X_pep = []\n",
    "for i in range(len(X_seq)):\n",
    "    seq = X_seq[i]\n",
    "    p1 = X_cl[i]\n",
    "    pep = []\n",
    "    min = p1-window_size\n",
    "    max = p1+window_size\n",
    "    for j in range(min,max):\n",
    "        if j < 0:\n",
    "            pep.append('X')\n",
    "        elif j >= len(seq):\n",
    "            pep.append('X')\n",
    "        else:\n",
    "            pep.append(seq[j])\n",
    "    X_pep.append(pep)\n",
    "\n",
    "#tranformation of peptides into vocab\n",
    "vocab = ['A','B','C','D','E','F','G','H','I','K','L','M','N','0','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "vocab_dict = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_dict[vocab[i]] = i\n",
    "\n",
    "X_pep_int = []\n",
    "for i in range(len(X_pep)):\n",
    "    pep = X_pep[i]\n",
    "    pep_int = []\n",
    "    for j in range(len(pep)):\n",
    "        pep_int.append(vocab_dict[pep[j]])\n",
    "    X_pep_int.append(pep_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding of enzyme names\n",
    "list_enzymes = list(set(y_name))\n",
    "dic_enzyme = {}\n",
    "for i in range(len(list_enzymes)):\n",
    "    dic_enzyme[list_enzymes[i]] = i\n",
    "\n",
    "y_name_int = []\n",
    "for i in range(len(y_name)):\n",
    "    y_encoded = [0]*len(list_enzymes)\n",
    "    y_encoded[dic_enzyme[y_name[i]]] = 1\n",
    "    y_name_int.append(y_encoded)\n",
    "\n",
    "#enregistrement of dic_enzyme\n",
    "\n",
    "with open('dic_enzyme.json', 'w') as fp:\n",
    "    json.dump(dic_enzyme, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[6, 3, 20, 4, 9, 6, 9, 9, 8, 5, 20, 15, 9, 2, 0, 15, 2, 7, 18, 20]\n",
      "MGDVEKGKKIFVQKCAQCHTVEKGGKHKTGPNLHGLFGRKTGQAPGFTYTDANKNKGITWKEETLMEYLENPKKYIPGTKMIFAGIKKKTEREDLIAYLKKATNE\n",
      "11\n",
      "A01.001\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0.05999755859375, 0.164306640625, 0.005794525146484375, -0.04986572265625, -0.005352020263671875, 0.1055908203125, -0.024169921875, -0.057769775390625, 0.039886474609375, -0.013580322265625, -0.045623779296875, 0.004024505615234375, -0.05499267578125, -0.00392913818359375, 0.0537109375, 0.04925537109375, 0.01137542724609375, -0.056182861328125, -0.04815673828125, -0.0511474609375, -0.0257415771484375, 0.058929443359375, 0.015655517578125, -0.053253173828125, -0.08770751953125, -0.0037364959716796875, 0.01125335693359375, -0.09478759765625, 0.0151824951171875, 0.036041259765625, -0.056488037109375, 0.038299560546875, -0.10797119140625, -0.1087646484375, -0.11962890625, -0.0174102783203125, -0.03564453125, 0.012969970703125, 0.0169219970703125, -0.00815582275390625, -0.060791015625, -0.044708251953125, -0.07855224609375, 0.0194549560546875, -0.040374755859375, 0.0174407958984375, -0.0062408447265625, 0.006572723388671875, -0.07891845703125, -0.00147247314453125, 0.0316162109375, 0.01419830322265625, -0.035125732421875, -0.095458984375, -0.0122528076171875, 0.024383544921875, 0.06134033203125, 0.021942138671875, 0.0465087890625, -0.006130218505859375, -0.036468505859375, -0.005802154541015625, -0.0011606216430664062, -0.071533203125, 0.039459228515625, 0.06494140625, -0.08819580078125, -0.01062774658203125, -0.07513427734375, -0.0102386474609375, -0.0094451904296875, -0.003803253173828125, -0.1656494140625, -0.114013671875, -0.035491943359375, 0.11236572265625, -0.00135040283203125, -0.0167999267578125, -0.01511383056640625, 0.055633544921875, 0.03704833984375, -0.050140380859375, -0.0517578125, 0.012054443359375, -0.0005240440368652344, -0.0777587890625, 0.06634521484375, -0.0345458984375, -0.0633544921875, 0.10516357421875, -0.0460205078125, 0.020355224609375, -0.006500244140625, -0.0284576416015625, -0.0831298828125, -0.03973388671875, -0.038543701171875, 0.0545654296875, 0.0552978515625, 0.016876220703125, -0.0889892578125, 0.0250701904296875, 0.047119140625, 0.0615234375, 0.0345458984375, 0.0372314453125, 0.0242767333984375, 0.006877899169921875, -0.06488037109375, -0.09423828125, -0.0126953125, -0.04193115234375, -0.07891845703125, -0.0675048828125, -0.0631103515625, 0.011383056640625, 0.01534271240234375, -0.042999267578125, 0.04302978515625, -0.070068359375, -0.046661376953125, -0.037109375, 0.0919189453125, -0.062042236328125, -0.05413818359375, 0.04132080078125, 0.03515625, -0.0809326171875, 0.052490234375, -0.0230865478515625, 0.01546478271484375, 0.01050567626953125, -0.03253173828125, -0.0660400390625, 0.0308074951171875, -0.11077880859375, 0.047454833984375, 0.0122528076171875, 0.00267791748046875, 0.005741119384765625, 0.06939697265625, -0.0162811279296875, 0.043548583984375, 0.018524169921875, -0.0260009765625, -0.0487060546875, 0.10400390625, -0.0236358642578125, -0.02728271484375, 0.05999755859375, -0.0080413818359375, -0.07354736328125, -0.09210205078125, 0.0200958251953125, -0.041778564453125, 0.002185821533203125, 0.0248565673828125, -0.051544189453125, -0.0052032470703125, -0.072265625, -0.0179443359375, -0.075927734375, -0.07330322265625, -0.0172119140625, -0.0050048828125, -0.058319091796875, 0.053863525390625, -0.02069091796875, -0.02069091796875, 0.0214996337890625, -0.004947662353515625, -0.0007147789001464844, 0.01580810546875, -0.0206146240234375, 0.026641845703125, 0.03167724609375, 0.06732177734375, -0.0118255615234375, -0.00908660888671875, -0.046356201171875, -0.0102996826171875, 0.0482177734375, -0.0245361328125, 0.0260772705078125, -0.004730224609375, 0.060211181640625, -0.00836181640625, 0.0158233642578125, -0.1097412109375, -0.054412841796875, 0.107421875, 0.029998779296875, 0.0239105224609375, 0.04901123046875, -0.0102691650390625, 0.07440185546875, -0.055999755859375, 0.012481689453125, 0.04364013671875, -0.01493072509765625, -0.1282958984375, 0.025665283203125, -0.06640625, -0.0887451171875, 0.031951904296875, -0.031280517578125, 0.2103271484375, 0.0428466796875, 0.006450653076171875, 0.01025390625, 0.1365966796875, -0.07293701171875, 0.024749755859375, 0.028961181640625, -0.00637054443359375, 0.0328369140625, 0.017974853515625, -0.0204620361328125, -0.01279449462890625, 0.014068603515625, 0.019317626953125, 0.0018033981323242188, 0.028839111328125, -0.0140228271484375, -0.04547119140625, 0.0173187255859375, -0.0174713134765625, -0.0294189453125, 0.09613037109375, 0.0171051025390625, 0.041229248046875, 0.0447998046875, -0.048736572265625, -0.040771484375, -0.01116943359375, 0.01904296875, -0.0006937980651855469, 0.08038330078125, 0.059600830078125, 0.0242462158203125, -0.037841796875, 0.06243896484375, 0.0200958251953125, -0.0090789794921875, -0.03607177734375, 0.018402099609375, 0.0160980224609375, -0.0219573974609375, 0.0090484619140625, 0.048828125, 0.0201416015625, -0.068115234375, -0.0234527587890625, 0.0183868408203125, -0.01175689697265625, 0.02825927734375, 0.067138671875, -0.060333251953125, -0.055023193359375, 0.050140380859375, -0.05517578125, -0.0035037994384765625, 0.068115234375, 0.128173828125, 0.00255584716796875, -0.00701141357421875, -0.05975341796875, 0.0018720626831054688, -0.060028076171875, -0.006656646728515625, -0.043365478515625, 0.0504150390625, -0.050262451171875, -0.1165771484375, 0.0430908203125, -0.011627197265625, -0.020050048828125, -0.0771484375, -0.0477294921875, -0.1502685546875, -0.0384521484375, 0.08599853515625, -0.0221405029296875, 0.02752685546875, 0.00472259521484375, 0.03582763671875, 0.034088134765625, 0.049468994140625, -0.004085540771484375, 0.025299072265625, 0.0276947021484375, 0.1195068359375, 0.01508331298828125, -0.07647705078125, 0.1455078125, -0.0119171142578125, 0.00104522705078125, -0.02606201171875, 0.07403564453125, 0.054656982421875, -0.003932952880859375, 0.05731201171875, -0.058624267578125, 0.034759521484375, -0.03106689453125, -0.0218048095703125, -0.025238037109375, 0.033721923828125, -0.0531005859375, 0.02069091796875, 0.044921875, 0.01519012451171875, -0.043701171875, 0.0975341796875, 0.03564453125, -0.0614013671875, -0.0614013671875, 0.0491943359375, -0.039642333984375, 0.038116455078125, 0.0020999908447265625, -0.08709716796875, -0.0200042724609375, 0.033233642578125, 0.1087646484375, 0.019561767578125, -0.034210205078125, -0.0003440380096435547, -0.03021240234375, -0.017303466796875, -0.017791748046875, -0.0240325927734375, 0.0274200439453125, -0.031524658203125, -0.00220489501953125, -0.033233642578125, 0.009033203125, 0.0038166046142578125, -0.0936279296875, 0.0709228515625, -0.0187225341796875, -0.01096343994140625, -0.0394287109375, 0.03778076171875, 0.0546875, 0.08612060546875, -0.0479736328125, -0.028839111328125, -0.01226806640625, 0.1285400390625, -0.0170135498046875, -0.042572021484375, -0.03265380859375, -0.0290374755859375, 0.035888671875, -0.06036376953125, -0.037689208984375, 0.03765869140625, -0.0364990234375, 0.01739501953125, -0.006011962890625, 0.049774169921875, 0.006114959716796875, 0.004268646240234375, 0.01476287841796875, 0.063232421875, -0.0712890625, 0.05706787109375, -0.0182037353515625, -0.03857421875, -0.01338958740234375, 0.052520751953125, -0.0305328369140625, -0.033905029296875, 0.027587890625, -0.006397247314453125, 0.01398468017578125, -0.0150146484375, 0.0467529296875, 0.030029296875, -0.04266357421875, 0.01324462890625, 0.0236358642578125, 0.047607421875, -0.00395965576171875, 0.0035953521728515625, 0.041412353515625, -0.1307373046875, -0.027557373046875, 0.0611572265625, 0.0233612060546875, 0.004474639892578125, -0.0085906982421875, -0.11138916015625, -0.005496978759765625, 0.07513427734375, -0.01788330078125, 0.01432037353515625, 0.004405975341796875, -0.061676025390625, -0.01318359375, 0.0117340087890625, 0.1939697265625, 0.0202789306640625, -0.00395965576171875, -0.08941650390625, 0.00865936279296875, 0.0204925537109375, -0.07989501953125, -0.040008544921875, -0.0246429443359375, -0.027984619140625, -0.047576904296875, -0.000263214111328125, -0.06488037109375, -0.02813720703125, -0.040557861328125, -0.02099609375, -0.084228515625, -0.01312255859375, 0.0303955078125, 0.03961181640625, -0.0469970703125, -0.059234619140625, 0.056671142578125, -0.020477294921875, -0.05645751953125, 0.002872467041015625, -0.043487548828125, -0.031036376953125, -0.0019626617431640625, -0.06988525390625, 0.0025463104248046875, 0.052764892578125, -0.0042266845703125, 0.01392364501953125, -0.060638427734375, 0.04510498046875, 0.00341033935546875, -0.058685302734375, -0.028045654296875, 0.0254058837890625, 0.015594482421875, 0.130126953125, 0.0285797119140625, -0.00830841064453125, -0.01702880859375, -0.022216796875, 0.01458740234375, -0.042633056640625, 0.05792236328125, -0.01641845703125, 0.00905609130859375, -0.0183258056640625, 0.07244873046875, 0.002880096435546875, -0.0325927734375, -0.050445556640625, -0.0203094482421875, -0.038818359375, -0.053619384765625, 0.072509765625, 0.0751953125, -0.0103302001953125, -0.01076507568359375, -0.006900787353515625, 0.004261016845703125, -0.004970550537109375, 0.036590576171875, 0.037506103515625, -0.043975830078125, 0.03436279296875, 0.0396728515625, 0.001903533935546875, 0.00554656982421875, 0.052703857421875, -0.01029205322265625, 0.0108642578125, -0.003971099853515625, 0.01277923583984375, -0.08709716796875, 0.0809326171875, 0.006649017333984375, 0.006748199462890625, -0.006259918212890625, -0.09637451171875, -0.0189208984375, -0.01390838623046875, -0.036651611328125, -0.0540771484375, -0.0247955322265625, 0.0650634765625, 0.027313232421875, -0.01380157470703125, 0.0136871337890625, -0.1011962890625, -0.038787841796875, 0.021026611328125, -0.0489501953125, -0.00273895263671875, -0.013580322265625, -0.01395416259765625, 0.00698089599609375, -0.0780029296875, -0.0009598731994628906, -0.06329345703125, 0.0274200439453125, 0.04022216796875, 0.12030029296875, 0.0280609130859375, 0.021514892578125, 0.0888671875, 0.03369140625, -0.032257080078125, -0.035125732421875, 0.0265045166015625, 0.09515380859375, -0.06903076171875, -0.02740478515625, -0.0025691986083984375, -0.021759033203125, 0.1580810546875, 0.035614013671875, 0.0780029296875, -0.03033447265625, -0.02410888671875, 0.040771484375, -0.0270233154296875, -0.041015625, -0.03985595703125, -0.04779052734375, -0.006683349609375, 0.0546875, 0.06024169921875, 0.0472412109375, 0.0249176025390625, -0.058441162109375, -0.09503173828125, -0.047149658203125, -0.007305145263671875, 0.02325439453125, 0.055145263671875, 0.0290679931640625, -0.061920166015625, 0.00457763671875, -0.063232421875, 0.0594482421875, 0.034088134765625, 0.0284423828125, -0.0295257568359375, -0.049072265625, -0.0052947998046875, 0.0032100677490234375, -0.0194854736328125, -0.0175628662109375, 0.00040030479431152344, -0.0780029296875, -0.050872802734375, 0.006946563720703125, -0.006870269775390625, -0.0963134765625, 0.0033721923828125, 0.02191162109375, 0.00785064697265625, 0.026580810546875, 0.04193115234375, -0.00875091552734375, -0.021087646484375, 0.00864410400390625, 0.0302276611328125, -0.0176239013671875, 0.05914306640625, 0.03070068359375, -0.01004791259765625, -0.059478759765625, 0.038116455078125, -0.0277099609375, 0.038330078125, 0.017242431640625, -0.03863525390625, 0.054443359375, -0.0113525390625, -0.059906005859375, -0.015106201171875, 0.00054931640625, 0.01099395751953125, -0.000446319580078125, -0.005950927734375, 0.10052490234375, -0.0501708984375, -0.03704833984375, -0.07696533203125, -0.0196533203125, 0.057220458984375, -0.0022640228271484375, 0.0343017578125, 0.03155517578125, 0.10260009765625, 0.08380126953125, 0.021728515625, -0.02166748046875, -0.0428466796875, -0.053466796875, 0.03936767578125, -0.040985107421875, -0.0031642913818359375, 0.10430908203125, -0.053192138671875, -0.05419921875, 0.0161895751953125, 0.004955291748046875, -0.01824951171875, -0.035888671875, 0.0216064453125, 0.1107177734375, 0.141357421875, -0.125, -0.042999267578125, 0.049285888671875, -0.04986572265625, -0.062164306640625, 0.039825439453125, -0.0726318359375, 0.051971435546875, 0.0474853515625, -0.054901123046875, 0.041168212890625, -0.002895355224609375, 0.015777587890625, -0.022125244140625, 0.036346435546875, 0.037353515625, 0.050994873046875, -0.006702423095703125, 0.07073974609375, 0.0182647705078125, -0.039337158203125, 0.0288238525390625, -0.126708984375, 0.0059967041015625, -0.08807373046875, 0.0118408203125, -0.0196075439453125, 0.035491943359375, -0.040374755859375, -0.02252197265625, -0.003696441650390625, 0.01287841796875, -0.10894775390625, 0.00015473365783691406, 0.032745361328125, 0.01108551025390625, -0.0170745849609375, 0.07769775390625, -0.03790283203125, -0.047454833984375, -0.006893157958984375, 0.034942626953125, 0.00554656982421875, -0.0643310546875, 0.0238037109375, -0.06964111328125, 0.0264434814453125, -0.051544189453125, 0.054901123046875, 0.056793212890625, -0.1094970703125, 0.01534271240234375, 0.04632568359375, 0.01407623291015625, 0.0531005859375, 0.00955963134765625, -0.00872039794921875, -0.053436279296875, -0.00787353515625, -0.08746337890625, -0.076416015625, 0.0229949951171875, -0.0159149169921875, 0.0404052734375, -1.4007091522216797e-05, -0.0750732421875, -0.054473876953125, 0.059844970703125, -0.0085601806640625, 0.035552978515625, 0.010650634765625, -0.0308990478515625, 0.051422119140625, 0.007564544677734375, -0.052154541015625, -0.001010894775390625, -0.02508544921875, -0.022705078125, -0.047119140625, 0.022216796875, 0.050140380859375, 0.08123779296875, -0.06414794921875, 0.1046142578125, -0.02362060546875, -0.028350830078125, 0.048828125, 0.0677490234375, -0.067138671875, -0.0108184814453125, 0.01062774658203125, 0.0523681640625, -0.00630950927734375, -0.01532745361328125, 0.051666259765625, 0.0191802978515625, 0.01027679443359375, -0.0236968994140625, 0.035400390625, -0.022613525390625, 0.017578125, 0.05828857421875, -0.01416015625, -0.0159912109375, 0.047149658203125, -0.062286376953125, -0.005046844482421875, 0.0026912689208984375, 0.04876708984375, -0.021881103515625, -0.024200439453125, 0.03424072265625, 0.0025691986083984375, 0.017547607421875, -0.0694580078125, -0.035614013671875, -0.1363525390625, 0.047576904296875, -0.025909423828125, 0.0253448486328125, 0.00797271728515625, -0.0687255859375, -0.059326171875, 0.05963134765625, 0.051971435546875, -0.015777587890625, 0.01201629638671875, 0.003215789794921875, 0.12213134765625, -0.0238189697265625, 0.032928466796875, -0.038909912109375, 0.042877197265625, -0.0243682861328125, -0.06646728515625, 0.1947021484375, 0.03717041015625, -0.11383056640625, 0.0248260498046875, -0.01114654541015625, 0.033416748046875, 0.029876708984375, -0.00875091552734375, 0.022674560546875, -0.0188751220703125, -0.04150390625, -0.02008056640625, 0.05810546875, 0.1339111328125, -0.0565185546875, 0.03887939453125, -0.0271759033203125, 0.06256103515625, -0.0345458984375, 0.00873565673828125, 0.01324462890625, -0.1270751953125, -0.014862060546875, -0.00788116455078125, 0.0187530517578125, -0.08428955078125, -0.00913238525390625, 0.02191162109375, 0.048736572265625, 0.006732940673828125, -0.001323699951171875, -0.049407958984375, 0.04754638671875, 0.0281219482421875, -0.00225067138671875, 0.002597808837890625, -0.0160369873046875, -0.023101806640625, 0.00917816162109375, 0.01397705078125, 0.01525115966796875, -0.1722412109375, 0.046478271484375, -0.0809326171875, 0.0113677978515625, 0.0085296630859375, 0.031280517578125, -0.018646240234375, 0.01430511474609375, 0.01092529296875, 0.0127716064453125, -0.0309906005859375, 0.043182373046875, 0.0272369384765625, 0.0004496574401855469, -0.00820159912109375, -0.0882568359375, -0.02587890625, 0.00743865966796875, -0.005794525146484375, 0.039947509765625, 0.02008056640625, 0.049530029296875, 0.1240234375, 0.0166168212890625, 0.09564208984375, -0.0019197463989257812, 0.072265625, 0.0184478759765625, -0.042144775390625, 0.079345703125, 0.06683349609375, -0.043609619140625, 0.003086090087890625, 0.0017604827880859375, -0.0140380859375, -0.030242919921875, -0.00977325439453125, -0.012908935546875, 0.0091705322265625, -0.004108428955078125, 0.03167724609375, 0.0762939453125, -0.01169586181640625, -0.01056671142578125, 0.08984375, 0.1376953125, -0.02777099609375, -0.12164306640625, -0.04937744140625, -0.05792236328125, 0.0780029296875, 0.0665283203125, 0.06292724609375, 0.0838623046875, -0.07318115234375, -0.042724609375, 0.036346435546875, 0.0083160400390625, 0.039031982421875, -0.0406494140625, 0.0302734375, -0.00357818603515625, 0.01025390625, 0.06158447265625, -0.03314208984375, -0.00994873046875, -0.0254974365234375, -0.053192138671875, -0.06689453125, 0.05047607421875, -0.012786865234375, 0.031280517578125, -0.01299285888671875, 0.003925323486328125, -0.07550048828125, 0.06561279296875, -0.06396484375, -0.09173583984375, -0.033203125, -0.08544921875, 0.0252227783203125, -0.0159454345703125, -0.0313720703125, -0.031707763671875, -0.0106658935546875, 0.0012836456298828125, 0.01078033447265625, 0.01143646240234375, 0.020172119140625, 0.042755126953125, -0.043609619140625, 0.0024967193603515625, 0.032501220703125, -0.04193115234375, -0.036376953125, -0.005855560302734375, 0.0210418701171875, -0.0032825469970703125, -0.0318603515625, -0.037078857421875, 0.0416259765625, -0.052642822265625, -0.040740966796875, 0.0181427001953125, -0.0024871826171875, 0.01291656494140625, 0.1131591796875, -0.07293701171875, -0.04730224609375, 0.07794189453125, -0.1591796875, 0.08880615234375, -0.049591064453125, 0.04058837890625, -0.01325225830078125, 0.08721923828125, -0.0033626556396484375, 0.043853759765625, 0.051025390625, 0.0279541015625, 0.08880615234375, 0.09832763671875, 0.007503509521484375, -0.05950927734375, 0.00870513916015625, -0.12310791015625, 0.037994384765625, 0.0209197998046875, -0.04840087890625, 0.03411865234375, 0.03564453125, 0.05267333984375, -0.048980712890625, -0.08807373046875, 0.039276123046875, -0.0704345703125, -0.021759033203125, -0.0027179718017578125, -0.00202178955078125, 0.0003139972686767578, 0.03631591796875, 0.0176544189453125, -0.002964019775390625, 0.045684814453125, 0.09100341796875, -0.046051025390625, 0.0167236328125, -0.06951904296875, -0.072509765625, 0.00299072265625, 0.0199432373046875, -0.033203125, 0.08856201171875, 0.019989013671875, -0.0197296142578125, -0.1163330078125, 0.001323699951171875, -0.086669921875, 0.006153106689453125, -0.00191497802734375, -0.0193634033203125, -0.159423828125, -0.02972412109375, -0.034637451171875, 0.037567138671875, 0.0259246826171875, 0.059112548828125, -0.037567138671875, -0.0036144256591796875, 0.0609130859375, -0.04583740234375, 0.055572509765625, -0.035400390625, -0.127685546875, 0.06707763671875, 0.052642822265625, -0.006565093994140625, 0.027679443359375, 0.08294677734375, -0.005100250244140625, -0.09930419921875, 0.0074462890625, -0.005840301513671875, 0.1298828125, -0.0100250244140625, 0.08203125, -0.035980224609375, -0.0099029541015625, -0.02813720703125, 0.0165863037109375, 0.0113372802734375, -0.06488037109375, 0.0662841796875, -0.041107177734375, -0.068359375, 0.005290985107421875, -0.0031585693359375, 0.01387786865234375, 0.050628662109375, -0.0234222412109375, 0.08648681640625, 0.01265716552734375, -0.021514892578125, -0.048858642578125, -0.01120758056640625, 0.02685546875, 0.07275390625, 0.01690673828125, -0.008148193359375, -0.0369873046875, 0.008941650390625, -0.0134429931640625, 0.1307373046875, 0.018707275390625, 0.093017578125, 0.037200927734375, 0.0110015869140625, -0.03717041015625, -0.045501708984375, -0.044708251953125, 0.048126220703125, 0.0166473388671875, -0.06866455078125, -0.0207672119140625, -0.035125732421875, -0.050384521484375, 0.0256805419921875, -0.10186767578125, -0.08807373046875, -0.0777587890625, 0.033935546875, 0.032745361328125]\n",
      "['GO:0070069', 'GO:0005829', 'GO:0005758', 'GO:0070469', 'GO:0009055', 'GO:0020037', 'GO:0042802', 'GO:0008289', 'GO:0046872', 'GO:0006915', 'GO:0018063', 'GO:0006123', 'GO:0006122', 'GO:0043065', 'GO:2001056', 'GO:0043280']\n"
     ]
    }
   ],
   "source": [
    "print(y_index[0])\n",
    "print(X_pep_int[0])\n",
    "print(X_seq[0])\n",
    "print(X_cl[0])\n",
    "print(y_name[0])\n",
    "print(y_name_int[0])\n",
    "print(X_embedding[0])\n",
    "print(X_GO[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_embedding_t = torch.tensor(X_embedding, dtype=torch.float32).to(device)\n",
    "X_pep_t = torch.tensor(X_pep_int, dtype=torch.int64).to(device)\n",
    "X_cl_t = torch.tensor(X_cl, dtype=torch.float32).to(device)\n",
    "X_cl_t = X_cl_t.unsqueeze(1).to(device)\n",
    "X_go_t = torch.tensor(X_GO_filtered_int, dtype=torch.float32).to(device)\n",
    "y_t = torch.tensor(y_name_int, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_embedding_pep_site(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim):\n",
    "        super(model_embedding_pep_site, self).__init__()\n",
    "        self.fc_embed = nn.Linear(embed_dim, 2048)\n",
    "        self.embed = nn.Embedding(26, 128)\n",
    "        self.gru = nn.GRU(128, 256, 2, batch_first=True)\n",
    "        self.fc_pep = nn.Linear(256, 2048)\n",
    "        self.fc_site = nn.Linear(1, 10)\n",
    "        self.fc1 = nn.Linear(2048 * 2 + 10, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "    \n",
    "    def forward(self, embed, pep, site):\n",
    "        x_embed = F.relu(self.fc_embed(embed))\n",
    "        x_pep = self.embed(pep)\n",
    "        x_pep, _ = self.gru(x_pep)\n",
    "        x_pep = x_pep[:,-1,:]\n",
    "        x_pep = F.relu(self.fc_pep(x_pep))\n",
    "        x_site = F.relu(self.fc_site(site))\n",
    "        x = torch.cat((x_embed, x_pep, x_site), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class model_embedding_pep_go_site(nn.Module):\n",
    "    def __init__(self, embed_dim, go_dim, output_dim):\n",
    "        super(model_embedding_pep_go_site, self).__init__()\n",
    "        self.fc_embed = nn.Linear(embed_dim, 2048)\n",
    "        self.embed = nn.Embedding(26, 128)\n",
    "        self.gru = nn.GRU(128, 256, 2, batch_first=True)\n",
    "        self.fc_pep = nn.Linear(256, 2048)\n",
    "        self.fc_go = nn.Linear(go_dim, 2048)\n",
    "        self.fc_site = nn.Linear(1, 10)\n",
    "        self.fc1 = nn.Linear(2048 * 3 + 10, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "    \n",
    "    def forward(self, embed, pep, site, go):\n",
    "        x_embed = F.relu(self.fc_embed(embed))\n",
    "        x_pep = self.embed(pep)\n",
    "        x_pep, _ = self.gru(x_pep)\n",
    "        x_pep = x_pep[:,-1,:]\n",
    "        x_pep = F.relu(self.fc_pep(x_pep))\n",
    "        x_go = F.relu(self.fc_go(go))\n",
    "        x_site = F.relu(self.fc_site(site))\n",
    "        x = torch.cat((x_embed, x_pep, x_go, x_site), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, pep, site, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed, pep, site, go)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_with_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, pep, site, go, labels = data\n",
    "            outputs = model(embed, pep, site, go)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] in predicted3[i]:\n",
    "                    correct3 += 1\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train loss: 0.008 | Test loss: 0.006 | Test accuracy: 0.410 | Test top 3 accuracy: 0.597\n",
      "Epoch 2/100 | Train loss: 0.005 | Test loss: 0.005 | Test accuracy: 0.460 | Test top 3 accuracy: 0.666\n",
      "Epoch 3/100 | Train loss: 0.005 | Test loss: 0.005 | Test accuracy: 0.475 | Test top 3 accuracy: 0.696\n",
      "Epoch 4/100 | Train loss: 0.004 | Test loss: 0.005 | Test accuracy: 0.489 | Test top 3 accuracy: 0.712\n",
      "Epoch 5/100 | Train loss: 0.004 | Test loss: 0.005 | Test accuracy: 0.499 | Test top 3 accuracy: 0.720\n",
      "Epoch 6/100 | Train loss: 0.004 | Test loss: 0.004 | Test accuracy: 0.516 | Test top 3 accuracy: 0.741\n",
      "Epoch 7/100 | Train loss: 0.004 | Test loss: 0.004 | Test accuracy: 0.530 | Test top 3 accuracy: 0.757\n",
      "Epoch 8/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.537 | Test top 3 accuracy: 0.764\n",
      "Epoch 9/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.542 | Test top 3 accuracy: 0.767\n",
      "Epoch 10/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.550 | Test top 3 accuracy: 0.778\n",
      "Epoch 11/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.543 | Test top 3 accuracy: 0.777\n",
      "Epoch 12/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.539 | Test top 3 accuracy: 0.777\n",
      "Epoch 13/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.540 | Test top 3 accuracy: 0.780\n",
      "Epoch 14/100 | Train loss: 0.002 | Test loss: 0.004 | Test accuracy: 0.541 | Test top 3 accuracy: 0.780\n",
      "Epoch 15/100 | Train loss: 0.002 | Test loss: 0.005 | Test accuracy: 0.538 | Test top 3 accuracy: 0.781\n",
      "Epoch 16/100 | Train loss: 0.002 | Test loss: 0.005 | Test accuracy: 0.538 | Test top 3 accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_pep_train, X_pep_test, X_cl_train, X_cl_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_pep_t, X_cl_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_pep_train, X_cl_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_pep_test, X_cl_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of enzymes = 535\n",
    "\n",
    "model = model_embedding_pep_go_site(embed_dim, go_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_with_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_with_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_pep_go_site_problem_1_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, pep, site, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed, pep, site)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_without_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, pep, site, go, labels = data\n",
    "            outputs = model(embed, pep, site)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                if labels[i] in predicted3[i]:\n",
    "                    correct3 += 1\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train loss: 0.008 | Test loss: 0.006 | Test accuracy: 0.390 | Test top 3 accuracy: 0.562\n",
      "Epoch 2/100 | Train loss: 0.006 | Test loss: 0.006 | Test accuracy: 0.413 | Test top 3 accuracy: 0.613\n",
      "Epoch 3/100 | Train loss: 0.005 | Test loss: 0.005 | Test accuracy: 0.443 | Test top 3 accuracy: 0.642\n",
      "Epoch 4/100 | Train loss: 0.005 | Test loss: 0.005 | Test accuracy: 0.471 | Test top 3 accuracy: 0.670\n",
      "Epoch 5/100 | Train loss: 0.005 | Test loss: 0.005 | Test accuracy: 0.487 | Test top 3 accuracy: 0.693\n",
      "Epoch 6/100 | Train loss: 0.004 | Test loss: 0.005 | Test accuracy: 0.510 | Test top 3 accuracy: 0.719\n",
      "Epoch 7/100 | Train loss: 0.004 | Test loss: 0.005 | Test accuracy: 0.519 | Test top 3 accuracy: 0.733\n",
      "Epoch 8/100 | Train loss: 0.004 | Test loss: 0.004 | Test accuracy: 0.534 | Test top 3 accuracy: 0.747\n",
      "Epoch 9/100 | Train loss: 0.004 | Test loss: 0.004 | Test accuracy: 0.536 | Test top 3 accuracy: 0.751\n",
      "Epoch 10/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.540 | Test top 3 accuracy: 0.763\n",
      "Epoch 11/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.543 | Test top 3 accuracy: 0.761\n",
      "Epoch 12/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.536 | Test top 3 accuracy: 0.761\n",
      "Epoch 13/100 | Train loss: 0.003 | Test loss: 0.004 | Test accuracy: 0.538 | Test top 3 accuracy: 0.764\n",
      "Epoch 14/100 | Train loss: 0.003 | Test loss: 0.005 | Test accuracy: 0.534 | Test top 3 accuracy: 0.764\n",
      "Epoch 15/100 | Train loss: 0.003 | Test loss: 0.005 | Test accuracy: 0.536 | Test top 3 accuracy: 0.766\n",
      "Epoch 16/100 | Train loss: 0.003 | Test loss: 0.005 | Test accuracy: 0.532 | Test top 3 accuracy: 0.764\n",
      "Epoch 17/100 | Train loss: 0.003 | Test loss: 0.005 | Test accuracy: 0.529 | Test top 3 accuracy: 0.764\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_pep_train, X_pep_test, X_cl_train, X_cl_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_pep_t, X_cl_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_pep_train, X_cl_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_pep_test, X_cl_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of enzymes = 535\n",
    "\n",
    "model = model_embedding_pep_site(embed_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_without_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_without_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_pep_site_problem_1_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
