{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.9/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ijson\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER\tSwissProt ID (E3)\tSwissProt ID (Substrate)\tSwissProt AC (E3)\tSwissProt AC (Substrate)\tGene Symbol (E3)\tGene Symbol (Substrate)\tSOURCE\tSOURCEID\tSENTENCE\tE3TYPE\tCOUNT\ttype\tspecies\n"
     ]
    }
   ],
   "source": [
    "#data recovery\n",
    "\n",
    "path = 'literature.E3.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "line_0 = data.split('\\n')[0]\n",
    "print(line_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression of incomplete lines\n",
    "\n",
    "E3 = []\n",
    "substrates = []\n",
    "\n",
    "for line in data.split('\\n')[1:]:\n",
    "    line = line.split('\\t')\n",
    "    try:\n",
    "        if line[3] != '-' and line[4] != '-':\n",
    "            E3.append(line[3][0:6])\n",
    "            substrates.append(line[4][0:6])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_E3:  842\n",
      "length of data after removing incomplete line:  4044\n",
      "example of list_E3:  ['Q9Y6I7', 'O95835', 'Q3U487', 'Q86TM6', 'Q9UDY8']\n"
     ]
    }
   ],
   "source": [
    "list_E3 = list(set(E3))\n",
    "print(\"length of list_E3: \", len(list_E3))\n",
    "print(\"length of data after removing incomplete line: \", len(E3))\n",
    "print(\"example of list_E3: \", list_E3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of list_substrates:  2486\n",
      "length of data after removing incomplete line:  4044\n",
      "example of list_substrates:  ['Q15555', 'Q9XYF4', 'O43541', 'P49458', 'Q9Y5K5']\n"
     ]
    }
   ],
   "source": [
    "list_substrates = list(set(substrates))\n",
    "print(\"length of list_substrates: \", len(list_substrates))\n",
    "print(\"length of data after removing incomplete line: \", len(substrates))\n",
    "print(\"example of list_substrates: \", list_substrates[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 570820\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n",
      "300000\n",
      "350000\n",
      "400000\n",
      "450000\n",
      "500000\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each substrate (embedding here)\n",
    "\n",
    "dic_sub = {}\n",
    "\n",
    "for i in range(len(list_substrates)):\n",
    "    dic_sub[list_substrates[i]] = []\n",
    "\n",
    "i = 0\n",
    "with h5py.File(\"../per-protein.h5\", \"r\") as file:\n",
    "    print(f\"number of entries: {len(file.items())}\")\n",
    "    for sequence_id, embedding in file.items():\n",
    "        i += 1\n",
    "        if i % 50000 == 0:\n",
    "            print(i)\n",
    "        if sequence_id in dic_sub:\n",
    "            dic_sub[sequence_id].append(np.array(embedding).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n"
     ]
    }
   ],
   "source": [
    "#recuperation of data on each substrate (GO and sequence here)\n",
    "\n",
    "i = 0\n",
    "with open('../uniprotkb_AND_reviewed_true_2024_03_26.json', \"rb\") as f:\n",
    "    for record in ijson.items(f, \"results.item\"):\n",
    "        try:\n",
    "            i += 1\n",
    "            refs = record.get(\"uniProtKBCrossReferences\", [])\n",
    "            if record[\"primaryAccession\"] in dic_sub:\n",
    "                GO = [ref[\"id\"] for ref in refs if ref.get(\"database\") == \"GO\"]\n",
    "                sequence = record[\"sequence\"][\"value\"]\n",
    "                dic_sub[record[\"primaryAccession\"]] = [dic_sub[record[\"primaryAccession\"]], GO, sequence]\n",
    "                    \n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "                \n",
    "        except Exception as record_error:\n",
    "            print(\"Error processing record:\", record_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of each output to keep the E3 for which we have enough data\n",
    "\n",
    "count_E3_dic = {}\n",
    "\n",
    "for i in range(len(list_E3)):\n",
    "    count_E3_dic[list_E3[i]] = 0\n",
    "\n",
    "for i in range(len(E3)):\n",
    "    count_E3_dic[E3[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acc = []    #accession number\n",
    "X_seq = []    #sequence\n",
    "X_GO = []     #GO\n",
    "X_emb = []    #embedding\n",
    "Y = []        #E3\n",
    "\n",
    "for i in range(len(substrates)):\n",
    "    if len(dic_sub[substrates[i]]) == 3 and len(dic_sub[substrates[i]][0]) > 0 and count_E3_dic[E3[i]] >= 5: #we keep only the E3 for which we have enough data\n",
    "        X_acc.append(substrates[i])\n",
    "        Y.append(E3[i])\n",
    "        X_emb.append(dic_sub[substrates[i]][0][0])\n",
    "        X_GO.append(dic_sub[substrates[i]][1])\n",
    "        X_seq.append(dic_sub[substrates[i]][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0362548828125, 0.10919189453125, -0.031890869140625, 0.04132080078125, 0.0831298828125, 0.0731201171875, 0.04193115234375, -0.110595703125, 0.043426513671875, -0.06585693359375, -0.0304718017578125, 0.002597808837890625, -0.0450439453125, -0.055511474609375, 0.0287017822265625, -0.03497314453125, 0.060699462890625, 0.058258056640625, 0.0038394927978515625, -0.044219970703125, -0.01068115234375, -0.007808685302734375, -0.0202178955078125, 0.0249481201171875, -0.0081634521484375, 0.039215087890625, 0.0058135986328125, -0.03350830078125, 0.03619384765625, 1.1086463928222656e-05, -0.0185699462890625, 0.06439208984375, -0.11669921875, 0.0584716796875, -0.1175537109375, -0.0640869140625, 0.0328369140625, -0.019927978515625, -0.037872314453125, 0.04486083984375, -0.041961669921875, 0.01537322998046875, -0.0198516845703125, 0.0240478515625, 0.056365966796875, -0.033843994140625, 0.002620697021484375, 0.060089111328125, 0.0246429443359375, 0.04595947265625, -0.0887451171875, -0.0419921875, 0.0215301513671875, -0.032318115234375, 0.08538818359375, -0.11053466796875, -0.034149169921875, -0.020751953125, -0.0689697265625, 0.04534912109375, -0.01329803466796875, -0.0382080078125, 0.005268096923828125, 0.04351806640625, 0.1024169921875, -0.0158233642578125, -0.11090087890625, -7.510185241699219e-05, -0.0006971359252929688, 0.004058837890625, -0.0199432373046875, 0.007610321044921875, -0.14990234375, 0.00916290283203125, 0.006473541259765625, -0.01535797119140625, 0.035247802734375, 0.07806396484375, 0.146484375, -0.016937255859375, 0.0184326171875, 0.05609130859375, -0.1123046875, -0.07073974609375, 0.0325927734375, -0.018157958984375, -0.00537109375, -0.016357421875, -0.0088958740234375, 0.027069091796875, 0.0186767578125, 0.019378662109375, 0.00464630126953125, 0.0394287109375, -0.0245361328125, -0.09759521484375, -0.027740478515625, 0.008392333984375, -0.044586181640625, -0.07196044921875, 0.07196044921875, 0.01239013671875, -0.03387451171875, 0.0284881591796875, 0.06005859375, 0.109619140625, -0.07098388671875, 0.034423828125, -0.047576904296875, -0.0584716796875, -0.0250091552734375, 0.0252532958984375, -0.06842041015625, 0.001499176025390625, 0.059783935546875, 0.0180511474609375, 0.07708740234375, 0.026031494140625, -0.010833740234375, -0.0161285400390625, -0.01385498046875, -0.09552001953125, -0.044036865234375, -0.006256103515625, 0.0030918121337890625, -0.053619384765625, -0.0203094482421875, 0.05914306640625, 0.04791259765625, 0.040863037109375, 0.04730224609375, -0.0157012939453125, 0.01422119140625, -0.1168212890625, -0.019683837890625, 0.03363037109375, -0.008026123046875, -0.00894927978515625, -0.045745849609375, -0.0259246826171875, 0.0689697265625, -0.0389404296875, -0.033050537109375, -0.00531768798828125, -0.03778076171875, 0.01064300537109375, -0.01434326171875, -0.0024585723876953125, -0.059539794921875, -0.09765625, 0.0249176025390625, -0.00946044921875, -0.0220489501953125, -0.012939453125, 0.020263671875, 0.01528167724609375, 0.11163330078125, 0.0232086181640625, 0.005001068115234375, 0.0151824951171875, -0.06121826171875, -0.010284423828125, -0.0261688232421875, -0.00890350341796875, -0.10064697265625, 0.08929443359375, 0.07879638671875, -0.0177154541015625, -0.0303802490234375, 0.07293701171875, -0.007625579833984375, 0.0030059814453125, 0.10247802734375, 0.056243896484375, -0.0024662017822265625, 0.061065673828125, -0.0870361328125, 0.02984619140625, -0.039581298828125, -0.0911865234375, -0.03277587890625, 0.06573486328125, -0.06719970703125, 0.0143890380859375, 0.0723876953125, -0.065673828125, 0.0021419525146484375, -0.09503173828125, -0.042694091796875, -0.0828857421875, 0.109375, 0.005184173583984375, 0.08514404296875, -0.030181884765625, 0.102783203125, 0.0035724639892578125, -0.053924560546875, 0.0252227783203125, 0.052947998046875, -0.0645751953125, -0.0236053466796875, 0.0743408203125, 0.03338623046875, -0.028106689453125, -0.025970458984375, -0.0141754150390625, 0.242431640625, -0.03167724609375, -0.033294677734375, -0.01326751708984375, -0.0311126708984375, -0.0458984375, -0.020843505859375, 0.03485107421875, -0.0036525726318359375, -0.032196044921875, -0.015716552734375, 0.0596923828125, 0.003631591796875, 0.056304931640625, -0.050506591796875, -0.033905029296875, -0.00567626953125, 0.09881591796875, -0.031280517578125, -0.035430908203125, 0.043304443359375, -0.0238037109375, -0.0247344970703125, 0.0296173095703125, -0.059661865234375, 0.0204315185546875, 0.0036334991455078125, 0.038238525390625, 0.00373077392578125, -0.06842041015625, -0.0531005859375, 0.0032024383544921875, 0.0673828125, 0.006378173828125, -0.08013916015625, 0.055023193359375, -0.084716796875, -0.1531982421875, 0.02435302734375, 0.0159454345703125, -0.058502197265625, -0.03424072265625, 0.0423583984375, -0.007648468017578125, -0.049835205078125, 0.006801605224609375, -0.01500701904296875, 0.040252685546875, 0.004940032958984375, 0.0285491943359375, 0.11187744140625, 0.01971435546875, -0.08837890625, 0.038848876953125, -0.0965576171875, -0.004169464111328125, -0.00952911376953125, 0.01153564453125, 0.0799560546875, 0.0025730133056640625, 0.00609588623046875, 0.05657958984375, 0.045440673828125, 0.042633056640625, 0.00740814208984375, -0.01058197021484375, -0.0509033203125, -0.12261962890625, 0.037200927734375, -0.0244293212890625, 0.0072784423828125, -0.08441162109375, -0.0682373046875, -0.09368896484375, 0.0513916015625, 0.019866943359375, -0.01904296875, -0.02117919921875, -0.0479736328125, -0.031463623046875, 0.001140594482421875, 0.0435791015625, -0.09674072265625, 0.06329345703125, 0.00893402099609375, 0.006931304931640625, 0.03533935546875, -0.085205078125, 0.03448486328125, -0.08282470703125, -0.02288818359375, -0.033538818359375, 0.0177001953125, -0.0260467529296875, 0.02655029296875, -0.07037353515625, -0.014678955078125, 0.012298583984375, -0.0199127197265625, 0.05859375, -0.02386474609375, -0.08489990234375, 0.031036376953125, 0.037994384765625, -0.0367431640625, 0.09503173828125, -0.047637939453125, -0.034210205078125, 0.038055419921875, -0.04974365234375, 0.02813720703125, 0.039459228515625, 0.0280609130859375, -0.0439453125, 0.0160064697265625, -0.10955810546875, -0.035736083984375, 0.06463623046875, -0.0767822265625, 0.0311126708984375, 0.002471923828125, 0.030120849609375, 0.070556640625, 0.0665283203125, -0.0673828125, 0.0152740478515625, 0.038177490234375, 0.01409149169921875, -0.1253662109375, 0.0322265625, -0.0206756591796875, -0.058837890625, 0.0107269287109375, 0.0164031982421875, -0.1260986328125, -0.004497528076171875, -0.03643798828125, 0.050323486328125, 0.0012388229370117188, 0.0053863525390625, 0.0161285400390625, 0.02484130859375, 0.0338134765625, 0.08392333984375, 0.06231689453125, -0.02386474609375, 0.006793975830078125, -0.042388916015625, 0.05792236328125, 0.0867919921875, -0.02862548828125, 0.038177490234375, -0.007045745849609375, 0.08709716796875, 0.07598876953125, -0.0191802978515625, 0.0628662109375, -0.06146240234375, -0.00981903076171875, 0.025665283203125, -0.12274169921875, 0.0131988525390625, 0.0029296875, -0.05078125, -0.0516357421875, -0.007022857666015625, -0.03228759765625, -0.0430908203125, -0.007770538330078125, -0.0263824462890625, -0.0025920867919921875, 0.08203125, 0.027191162109375, 0.142578125, -0.006103515625, 0.039215087890625, 0.0206298828125, 0.1295166015625, 0.0836181640625, 0.03070068359375, 0.034759521484375, 0.00910186767578125, -0.01029205322265625, -0.045196533203125, 0.055572509765625, -0.0311431884765625, -0.0244293212890625, -0.053955078125, 0.01177215576171875, 0.01442718505859375, 0.03253173828125, -0.09014892578125, -0.0268707275390625, -0.0784912109375, 0.0333251953125, 0.02587890625, 0.047271728515625, 0.08636474609375, -0.01025390625, -0.0986328125, 0.04852294921875, 0.08251953125, -0.003765106201171875, -0.0797119140625, -0.039703369140625, 0.004726409912109375, 0.005245208740234375, -0.0219879150390625, -0.08221435546875, 0.0919189453125, -0.0775146484375, -0.0162200927734375, 0.01508331298828125, 0.033966064453125, -0.05511474609375, -0.00485992431640625, 0.0132598876953125, 0.0838623046875, -0.0455322265625, -0.01064300537109375, 0.0189361572265625, -0.034332275390625, 0.007659912109375, -0.054229736328125, 0.0235443115234375, 0.05718994140625, -0.00856781005859375, 0.0274810791015625, -0.0146942138671875, -0.11700439453125, -0.0305633544921875, 0.1605224609375, -0.01788330078125, 0.0049896240234375, -0.0037994384765625, 0.1087646484375, 0.04681396484375, 0.11529541015625, -0.0174407958984375, -0.04229736328125, -0.031036376953125, -0.04168701171875, 0.06329345703125, 0.01678466796875, 0.0209197998046875, 0.042633056640625, -0.00458526611328125, 0.0236053466796875, 0.03717041015625, -0.0031757354736328125, 0.07135009765625, 0.1544189453125, 0.032440185546875, -0.064208984375, -0.0723876953125, -0.07806396484375, 0.05963134765625, -0.005466461181640625, 0.06988525390625, 0.00414276123046875, 0.034088134765625, -0.01739501953125, 0.0031833648681640625, 0.01340484619140625, -0.01153564453125, 0.0054779052734375, -0.021942138671875, -0.00792694091796875, -0.0914306640625, 0.023040771484375, -0.0276336669921875, 0.02593994140625, 0.0738525390625, -0.0675048828125, -0.022125244140625, 0.03564453125, -0.04498291015625, -0.0088348388671875, -0.0721435546875, 0.038848876953125, 0.088134765625, -0.03900146484375, -0.071044921875, 0.002040863037109375, 0.0160980224609375, 0.01444244384765625, 0.038055419921875, -0.0086517333984375, 0.08990478515625, 0.072998046875, 0.0272979736328125, 0.0235595703125, -0.040679931640625, -0.0163421630859375, -0.15380859375, 0.062225341796875, -0.035552978515625, -0.0814208984375, -0.043670654296875, -0.044189453125, -0.012725830078125, -0.0257568359375, -0.007106781005859375, 0.026702880859375, -0.061798095703125, -0.0107574462890625, 0.00600433349609375, -0.0034503936767578125, -0.0204010009765625, 0.03692626953125, 0.1175537109375, 0.04022216796875, 0.0111236572265625, 0.040740966796875, -0.01471710205078125, 0.09881591796875, -0.03338623046875, -0.006626129150390625, -0.005504608154296875, 0.072998046875, 0.00860595703125, -0.08880615234375, -0.0418701171875, -0.02545166015625, -0.048065185546875, -0.051544189453125, 0.05755615234375, -0.007053375244140625, -0.0743408203125, 0.020721435546875, 0.0201873779296875, -0.019775390625, -0.07806396484375, -0.0294647216796875, 0.08837890625, 0.068359375, -0.0178985595703125, -0.01218414306640625, 0.0013065338134765625, -0.0202484130859375, -0.00974273681640625, -0.1412353515625, -0.09686279296875, -0.005802154541015625, -0.099365234375, -0.036407470703125, -0.01389312744140625, -0.04449462890625, -0.056793212890625, -0.1544189453125, -0.0188751220703125, 0.0221405029296875, -0.023712158203125, 0.01497650146484375, 0.051971435546875, 0.037353515625, -0.0263671875, 0.0772705078125, 0.0283660888671875, -0.028472900390625, -0.051483154296875, 0.02484130859375, -0.0202484130859375, 0.0287017822265625, 0.047332763671875, -0.01418304443359375, 0.0511474609375, 0.008331298828125, 0.00885772705078125, 0.0218048095703125, -0.0028018951416015625, -0.01001739501953125, -0.01348876953125, 0.005748748779296875, -0.05322265625, -0.0286712646484375, -0.06085205078125, -0.0577392578125, -0.0206298828125, 0.0103912353515625, 0.092529296875, 0.056365966796875, -0.022430419921875, 0.031219482421875, -0.0662841796875, 0.0186614990234375, -0.0006241798400878906, -0.08868408203125, 0.0025920867919921875, -0.0083160400390625, -0.05767822265625, -0.0264129638671875, 0.06072998046875, 0.06597900390625, 0.024810791015625, -0.09796142578125, 0.0169525146484375, 0.05328369140625, 0.04608154296875, 0.036407470703125, -0.065673828125, -0.0291595458984375, 0.0255889892578125, 0.033447265625, 0.029052734375, 0.118408203125, -0.0018177032470703125, -0.0287017822265625, 0.00272369384765625, 0.1436767578125, 0.032684326171875, -0.01073455810546875, -0.008941650390625, 0.027130126953125, -0.0296783447265625, -0.040679931640625, 0.07073974609375, -0.0306396484375, 0.0171966552734375, 0.008758544921875, -0.050506591796875, 0.0684814453125, -0.031158447265625, 0.0093231201171875, 0.07952880859375, 0.10516357421875, 0.024139404296875, -0.0290985107421875, -0.013946533203125, 0.021636962890625, 0.01352691650390625, -0.0290985107421875, 0.004978179931640625, -0.040191650390625, 0.0293426513671875, -0.01776123046875, -0.0207977294921875, 0.05194091796875, -0.01155853271484375, 0.020233154296875, -0.0003535747528076172, 0.057464599609375, -0.054534912109375, -0.0182037353515625, 0.027069091796875, -0.06268310546875, 0.0789794921875, 0.0460205078125, -0.01236724853515625, 0.00643157958984375, 0.07635498046875, 0.005767822265625, -0.0181884765625, -0.01189422607421875, 0.020050048828125, -0.0543212890625, 0.0285491943359375, -0.08001708984375, 0.02252197265625, -0.0131378173828125, 0.056915283203125, 0.02325439453125, -0.0013647079467773438, 0.1181640625, 0.0360107421875, 0.0537109375, 0.058197021484375, 0.0212249755859375, 0.020263671875, -0.00994873046875, 0.019134521484375, -0.0139923095703125, -0.041229248046875, 0.051544189453125, -0.11822509765625, 0.0088653564453125, -0.0272064208984375, 0.037109375, -0.1019287109375, -0.02496337890625, -0.045562744140625, 0.036224365234375, -0.050628662109375, 0.00962066650390625, 0.027679443359375, -0.02093505859375, 0.10650634765625, 0.0494384765625, -0.00408172607421875, -0.0033550262451171875, 0.0589599609375, -0.01026153564453125, -0.04046630859375, -0.0273590087890625, -0.029296875, -0.00307464599609375, -0.0218505859375, -0.0037860870361328125, 0.11346435546875, 0.029388427734375, -0.032501220703125, -0.012451171875, 0.0435791015625, -0.0010595321655273438, 0.019256591796875, -0.0491943359375, -0.0277252197265625, -0.03936767578125, 0.024200439453125, -0.040069580078125, -0.0020542144775390625, 0.0158843994140625, 0.00592803955078125, -0.0014781951904296875, -0.078857421875, -0.0576171875, 0.029296875, -0.01318359375, 0.040130615234375, 0.00537109375, -0.042999267578125, -0.025390625, 0.0224151611328125, 0.080322265625, -0.08270263671875, -0.0345458984375, -0.0085296630859375, 0.00438690185546875, -0.0723876953125, -0.054962158203125, -0.028472900390625, -0.01169586181640625, -0.0333251953125, 0.01123809814453125, 0.0245513916015625, 0.065185546875, 0.036407470703125, -0.0015583038330078125, 0.022369384765625, -0.042327880859375, -0.0309600830078125, -0.0265045166015625, 0.08697509765625, -0.081787109375, -0.02667236328125, 0.0278778076171875, 0.0080108642578125, -0.01174163818359375, -0.032928466796875, 0.033447265625, -0.07073974609375, -2.1457672119140625e-05, -0.047149658203125, -0.054595947265625, 0.004047393798828125, -0.0494384765625, -0.005153656005859375, -0.025115966796875, -0.0196685791015625, 0.0266265869140625, 0.10894775390625, 0.1068115234375, -0.058868408203125, -0.0745849609375, -0.0496826171875, 0.014923095703125, 0.03436279296875, 0.01445770263671875, 0.0305328369140625, 0.00815582275390625, -0.017852783203125, 0.038299560546875, -0.06744384765625, 0.029510498046875, -0.012451171875, 0.072998046875, 0.0477294921875, -0.06158447265625, 0.07427978515625, -0.00812530517578125, -0.06158447265625, -0.1259765625, 0.0205078125, -0.033905029296875, 0.076416015625, -0.0257110595703125, -0.0165557861328125, -0.00351715087890625, -0.060943603515625, 0.03338623046875, -0.00838470458984375, 0.07025146484375, -0.054840087890625, 0.10406494140625, 0.053253173828125, -0.029998779296875, 0.05810546875, 0.030303955078125, -0.0299072265625, -0.0311126708984375, -0.0638427734375, 0.0478515625, 0.06439208984375, 0.026458740234375, 0.087646484375, 0.0240478515625, -0.026641845703125, 0.12017822265625, 0.1256103515625, 0.01311492919921875, -0.041107177734375, -0.036224365234375, 0.0307159423828125, 0.005733489990234375, -0.0189666748046875, 0.08465576171875, -0.05462646484375, 0.01568603515625, -0.0258331298828125, 0.10491943359375, 0.08294677734375, -0.019866943359375, -0.040863037109375, 0.01200103759765625, -0.04766845703125, 0.046417236328125, 0.0758056640625, 0.037384033203125, -0.037933349609375, -0.08282470703125, 0.042724609375, 0.035308837890625, -0.003993988037109375, -0.015380859375, -0.01120758056640625, -0.013275146484375, 0.05889892578125, -0.0140533447265625, -0.04052734375, -0.02398681640625, -0.11370849609375, -0.015625, -0.037750244140625, 0.1314697265625, 0.0458984375, -0.00572967529296875, 0.05010986328125, 0.007061004638671875, -0.0308990478515625, 0.0197601318359375, 0.049163818359375, 0.0183868408203125, 0.0195770263671875, -0.0173187255859375, -0.008392333984375, 0.029296875, 0.05615234375, -0.04962158203125, 0.031951904296875, -0.0136566162109375, -0.002155303955078125, 0.09539794921875, 0.0233612060546875, -0.12030029296875, -0.095458984375, 0.04376220703125, 0.048858642578125, -0.00887298583984375, 0.0687255859375, -0.09185791015625, -0.0496826171875, 0.0078125, 0.065673828125, -0.08587646484375, -0.0024623870849609375, 0.0298309326171875, 0.039398193359375, -0.0278472900390625, -0.008392333984375, -0.01447296142578125, 0.0022335052490234375, -0.0064239501953125, 0.0275421142578125, 0.038360595703125, 0.03436279296875, 0.0133209228515625, 0.09283447265625, -0.0689697265625, -0.1148681640625, -0.0181884765625, -0.01142120361328125, -0.01922607421875, 0.10992431640625, 0.01556396484375, -0.07928466796875, 0.0462646484375, -0.09881591796875, -0.052734375, -0.047271728515625, -0.043060302734375, 0.02886962890625, 0.095703125, 0.0804443359375, -0.037933349609375, 0.0198516845703125, -0.031890869140625, -0.035552978515625, -0.03448486328125, -0.01055145263671875, -0.0020809173583984375, -0.0341796875, -0.0096435546875, -0.0234527587890625, -0.029052734375, -0.052734375, 0.054229736328125, 0.01029205322265625, -0.009674072265625, 0.040618896484375, -0.0322265625, -0.0195770263671875, 0.0253448486328125, -0.001071929931640625, 0.058380126953125, 0.00734710693359375, -0.0443115234375, -0.019500732421875, 0.061920166015625, -0.058349609375, 0.051544189453125, 0.060943603515625, 0.02545166015625, -0.030670166015625, 0.01953125, 0.00988006591796875, 0.06512451171875, 0.0305938720703125, -0.04833984375, 0.06787109375, -0.063232421875, 0.04833984375, -0.11004638671875, -0.00989532470703125, 0.029449462890625, -0.047393798828125, 0.005779266357421875, 0.059173583984375, -0.080078125, -0.0855712890625, -0.049652099609375, -0.0196685791015625, 0.01055908203125, 0.0894775390625, -0.1612548828125, 0.04315185546875, 0.119140625, -0.06549072265625, -0.0192718505859375, 0.026123046875, -0.050537109375, 0.0160980224609375, -0.0200653076171875, 0.0933837890625, 0.03729248046875, 0.06610107421875, -0.08612060546875, -0.029022216796875, 0.059539794921875, 0.04364013671875, 0.010894775390625, 0.0291290283203125, 0.03900146484375, 0.00525665283203125, 0.000644683837890625, -0.0160369873046875, 0.036865234375, -0.0200958251953125, 0.00632476806640625, 0.0792236328125, -0.0279541015625, -0.03887939453125, 0.033477783203125, -0.028564453125, -0.00860595703125, -0.007778167724609375, 0.0220184326171875, 0.01837158203125, -0.0247039794921875, 0.03302001953125, -0.0799560546875, 0.0161590576171875, -0.042327880859375, -0.0582275390625, 0.01287078857421875, 0.0341796875, 0.0024013519287109375, 0.03338623046875, 0.0143585205078125, 0.1212158203125, 0.043426513671875, 0.029693603515625, 0.093994140625, 0.0014581680297851562, 0.009857177734375, 0.035491943359375, -0.07861328125, 0.07061767578125, -0.04595947265625, 0.0070953369140625, -0.004695892333984375, -0.0184478759765625, -0.0282745361328125, 0.07830810546875, -0.0684814453125, -0.025115966796875, 0.02435302734375, 0.044830322265625, 0.00988006591796875]\n",
      "['GO:0005634', 'GO:0003700', 'GO:1990841', 'GO:0043565', 'GO:0009740', 'GO:0009867', 'GO:0009585', 'GO:0009753', 'GO:0048443', 'GO:0080086']\n",
      "MEKRGGGSSGGSGSSAEAEVRKGPWTMEEDLILINYIANHGDGVWNSLAKSAGLKRTGKSCRLRWLNYLRPDVRRGNITPEEQLIIMELHAKWGNRWSKIAKHLPGRTDNEIKNFWRTRIQKYIKQSDVTTTSSVGSHHSSEINDQAASTSSHNVFCTQDQAMETYSPTPTSYQHTNMEFNYGNYSAAAVTATVDYPVPMTVDDQTGENYWGMDDIWSSMHLLNGN\n",
      "Q9LK95\n",
      "P43254\n",
      "2923\n"
     ]
    }
   ],
   "source": [
    "print(X_emb[0])\n",
    "print(X_GO[0])\n",
    "print(X_seq[0])\n",
    "print(X_acc[0])\n",
    "print(Y[0])\n",
    "print(len(X_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of go terms:  9355\n",
      "example:  [('GO:0005634', 1776), ('GO:0003700', 345), ('GO:1990841', 114), ('GO:0043565', 168), ('GO:0009740', 3)]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "#encoding of the go terms\n",
    "\n",
    "count_go = {}\n",
    "\n",
    "for i in range(len(X_GO)):\n",
    "    for go in X_GO[i]:\n",
    "        if go in count_go:\n",
    "            count_go[go] += 1\n",
    "        else:\n",
    "            count_go[go] = 1\n",
    "\n",
    "print(\"number of go terms: \", len(count_go))\n",
    "print(\"example: \", list(count_go.items())[0:5])\n",
    "\n",
    "#we keep the most frequent go terms\n",
    "number_go = 2000\n",
    "\n",
    "most_frequent_go = dict(sorted(count_go.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "most_frequent_go = dict(list(most_frequent_go.items())[0:number_go])\n",
    "\n",
    "X_GO_filtered = []\n",
    "\n",
    "for i in range(len(X_GO)):\n",
    "    go_filtered = []\n",
    "    for go in X_GO[i]:\n",
    "        if go in most_frequent_go:\n",
    "            go_filtered.append(go)\n",
    "    X_GO_filtered.append(go_filtered)\n",
    "\n",
    "# encoder X_GO_filtered\n",
    "\n",
    "list_go = list(most_frequent_go.keys())\n",
    "dic_GO = {}\n",
    "for i in range(len(list_go)):\n",
    "    dic_GO[list_go[i]] = i\n",
    "\n",
    "X_GO_filtered_int = []\n",
    "\n",
    "for i in range(len(X_GO_filtered)):\n",
    "    go = X_GO_filtered[i]\n",
    "    go_int = [0]*len(list_go)\n",
    "    for j in range(len(go)):\n",
    "        go_int[dic_GO[go[j]]] = 1\n",
    "    X_GO_filtered_int.append(go_int)\n",
    "\n",
    "print(X_GO_filtered_int[0])\n",
    "print(len(X_GO_filtered_int[0]))\n",
    "\n",
    "#enregistrement of dic_GO\n",
    "\n",
    "with open('dic_GO_problem_2.json', 'w') as fp:\n",
    "    json.dump(dic_GO, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding of E3 accession\n",
    "list_E3 = list(set(Y))\n",
    "\n",
    "dic_E3 = {}\n",
    "for i in range(len(list_E3)):\n",
    "    dic_E3[list_E3[i]] = i\n",
    "\n",
    "dic_substrates_E3 = {}\n",
    "for i in range(len(X_acc)):\n",
    "    dic_substrates_E3[X_acc[i]] = [0]*len(list_E3)\n",
    "\n",
    "for i in range(len(X_acc)):\n",
    "    dic_substrates_E3[X_acc[i]][dic_E3[Y[i]]] = 1\n",
    "\n",
    "Y_int = []\n",
    "for i in range(len(Y)):\n",
    "    Y_int.append(dic_substrates_E3[X_acc[i]])\n",
    "\n",
    "#enregistrement of dic_E3\n",
    "with open('dic_E3.json', 'w') as fp:\n",
    "    json.dump(dic_E3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_embedding_t = torch.tensor(X_emb, dtype=torch.float32).to(device)\n",
    "X_go_t = torch.tensor(X_GO_filtered_int, dtype=torch.float32).to(device)\n",
    "y_t = torch.tensor(Y_int, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_embedding(nn.Module):\n",
    "    def __init__(self, embed_dim, output_dim):\n",
    "        super(model_embedding, self).__init__()\n",
    "        self.fc = nn.Linear(embed_dim, 2048)\n",
    "        self.fc1 = nn.Linear(2048, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "        \n",
    "    def forward(self, embed):\n",
    "        x = F.relu(self.fc(embed))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "class model_embedding_go(nn.Module):\n",
    "    def __init__(self, embed_dim, go_dim, output_dim):\n",
    "        super(model_embedding_go, self).__init__()\n",
    "        self.fc_embed = nn.Linear(embed_dim, 2048)\n",
    "        self.fc_go = nn.Linear(go_dim, 2048)\n",
    "        self.fc1 = nn.Linear(4096, 2048)\n",
    "        self.fc2 = nn.Linear(2048, output_dim)\n",
    "    \n",
    "    def forward(self, embed, go):\n",
    "        x_embed = F.relu(self.fc_embed(embed))\n",
    "        x_go = F.relu(self.fc_go(go))\n",
    "        x = torch.cat((x_embed, x_go), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed, go)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_with_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, go, labels = data\n",
    "            outputs = model(embed, go)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(predicted)):\n",
    "                if labels[i][predicted[i]] == 1:\n",
    "                    correct += 1\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                for j in range(3):\n",
    "                    if labels[i][predicted3[i][j]] == 1:\n",
    "                        correct3 += 1\n",
    "                        break\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/antoine.du-pouget-de-nadaillac/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train loss: 0.080 | Test loss: 0.075 | Test accuracy: 0.202 | Test top 3 accuracy: 0.338\n",
      "Epoch 2/100 | Train loss: 0.071 | Test loss: 0.068 | Test accuracy: 0.323 | Test top 3 accuracy: 0.441\n",
      "Epoch 3/100 | Train loss: 0.061 | Test loss: 0.062 | Test accuracy: 0.378 | Test top 3 accuracy: 0.497\n",
      "Epoch 4/100 | Train loss: 0.056 | Test loss: 0.058 | Test accuracy: 0.398 | Test top 3 accuracy: 0.532\n",
      "Epoch 5/100 | Train loss: 0.051 | Test loss: 0.055 | Test accuracy: 0.450 | Test top 3 accuracy: 0.557\n",
      "Epoch 6/100 | Train loss: 0.048 | Test loss: 0.053 | Test accuracy: 0.477 | Test top 3 accuracy: 0.598\n",
      "Epoch 7/100 | Train loss: 0.045 | Test loss: 0.051 | Test accuracy: 0.523 | Test top 3 accuracy: 0.602\n",
      "Epoch 8/100 | Train loss: 0.043 | Test loss: 0.050 | Test accuracy: 0.526 | Test top 3 accuracy: 0.617\n",
      "Epoch 9/100 | Train loss: 0.041 | Test loss: 0.048 | Test accuracy: 0.540 | Test top 3 accuracy: 0.638\n",
      "Epoch 10/100 | Train loss: 0.043 | Test loss: 0.047 | Test accuracy: 0.571 | Test top 3 accuracy: 0.660\n",
      "Epoch 11/100 | Train loss: 0.038 | Test loss: 0.046 | Test accuracy: 0.586 | Test top 3 accuracy: 0.655\n",
      "Epoch 12/100 | Train loss: 0.037 | Test loss: 0.046 | Test accuracy: 0.590 | Test top 3 accuracy: 0.658\n",
      "Epoch 13/100 | Train loss: 0.036 | Test loss: 0.045 | Test accuracy: 0.593 | Test top 3 accuracy: 0.675\n",
      "Epoch 14/100 | Train loss: 0.035 | Test loss: 0.045 | Test accuracy: 0.591 | Test top 3 accuracy: 0.665\n",
      "Epoch 15/100 | Train loss: 0.034 | Test loss: 0.044 | Test accuracy: 0.614 | Test top 3 accuracy: 0.680\n",
      "Epoch 16/100 | Train loss: 0.034 | Test loss: 0.044 | Test accuracy: 0.607 | Test top 3 accuracy: 0.691\n",
      "Epoch 17/100 | Train loss: 0.033 | Test loss: 0.043 | Test accuracy: 0.605 | Test top 3 accuracy: 0.696\n",
      "Epoch 18/100 | Train loss: 0.032 | Test loss: 0.043 | Test accuracy: 0.605 | Test top 3 accuracy: 0.699\n",
      "Epoch 19/100 | Train loss: 0.031 | Test loss: 0.043 | Test accuracy: 0.614 | Test top 3 accuracy: 0.685\n",
      "Epoch 20/100 | Train loss: 0.030 | Test loss: 0.043 | Test accuracy: 0.617 | Test top 3 accuracy: 0.687\n",
      "Epoch 21/100 | Train loss: 0.030 | Test loss: 0.043 | Test accuracy: 0.621 | Test top 3 accuracy: 0.703\n",
      "Epoch 22/100 | Train loss: 0.030 | Test loss: 0.043 | Test accuracy: 0.617 | Test top 3 accuracy: 0.692\n",
      "Epoch 23/100 | Train loss: 0.029 | Test loss: 0.043 | Test accuracy: 0.615 | Test top 3 accuracy: 0.694\n",
      "Epoch 24/100 | Train loss: 0.029 | Test loss: 0.043 | Test accuracy: 0.614 | Test top 3 accuracy: 0.697\n",
      "Epoch 25/100 | Train loss: 0.028 | Test loss: 0.043 | Test accuracy: 0.629 | Test top 3 accuracy: 0.703\n",
      "Epoch 26/100 | Train loss: 0.028 | Test loss: 0.043 | Test accuracy: 0.629 | Test top 3 accuracy: 0.699\n",
      "Epoch 27/100 | Train loss: 0.028 | Test loss: 0.043 | Test accuracy: 0.621 | Test top 3 accuracy: 0.689\n",
      "Epoch 28/100 | Train loss: 0.028 | Test loss: 0.043 | Test accuracy: 0.615 | Test top 3 accuracy: 0.696\n",
      "Epoch 29/100 | Train loss: 0.027 | Test loss: 0.043 | Test accuracy: 0.612 | Test top 3 accuracy: 0.697\n",
      "Epoch 30/100 | Train loss: 0.027 | Test loss: 0.043 | Test accuracy: 0.621 | Test top 3 accuracy: 0.696\n",
      "Epoch 31/100 | Train loss: 0.027 | Test loss: 0.043 | Test accuracy: 0.609 | Test top 3 accuracy: 0.696\n",
      "Epoch 32/100 | Train loss: 0.027 | Test loss: 0.043 | Test accuracy: 0.619 | Test top 3 accuracy: 0.699\n",
      "Epoch 33/100 | Train loss: 0.027 | Test loss: 0.043 | Test accuracy: 0.617 | Test top 3 accuracy: 0.699\n",
      "Epoch 34/100 | Train loss: 0.026 | Test loss: 0.043 | Test accuracy: 0.615 | Test top 3 accuracy: 0.708\n",
      "Epoch 35/100 | Train loss: 0.026 | Test loss: 0.043 | Test accuracy: 0.617 | Test top 3 accuracy: 0.708\n",
      "Epoch 36/100 | Train loss: 0.026 | Test loss: 0.044 | Test accuracy: 0.617 | Test top 3 accuracy: 0.692\n",
      "Epoch 37/100 | Train loss: 0.026 | Test loss: 0.043 | Test accuracy: 0.626 | Test top 3 accuracy: 0.708\n",
      "Epoch 38/100 | Train loss: 0.026 | Test loss: 0.043 | Test accuracy: 0.621 | Test top 3 accuracy: 0.706\n",
      "Epoch 39/100 | Train loss: 0.030 | Test loss: 0.043 | Test accuracy: 0.615 | Test top 3 accuracy: 0.704\n",
      "Epoch 40/100 | Train loss: 0.026 | Test loss: 0.044 | Test accuracy: 0.619 | Test top 3 accuracy: 0.694\n",
      "Epoch 41/100 | Train loss: 0.026 | Test loss: 0.044 | Test accuracy: 0.615 | Test top 3 accuracy: 0.696\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of enzymes = 535\n",
    "\n",
    "model = model_embedding_go(embed_dim, go_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 15\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_with_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_with_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_go_problem_2_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_go(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        embed, go, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(embed)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def test_accuracy_without_go(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    correct3 = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            embed, go, labels = data\n",
    "            outputs = model(embed)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            for i in range(len(predicted)):\n",
    "                if labels[i][predicted[i]] == 1:\n",
    "                    correct += 1\n",
    "            #top 3 predictions\n",
    "            _, predicted3 = torch.topk(outputs.data, 3, dim=1)\n",
    "            for i in range(len(labels)):\n",
    "                for j in range(3):\n",
    "                    if labels[i][predicted3[i][j]] == 1:\n",
    "                        correct3 += 1\n",
    "                        break\n",
    "    return correct / total, correct3 / total, running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 | Train loss: 0.083 | Test loss: 0.080 | Test accuracy: 0.168 | Test top 3 accuracy: 0.262\n",
      "Epoch 2/150 | Train loss: 0.078 | Test loss: 0.077 | Test accuracy: 0.198 | Test top 3 accuracy: 0.347\n",
      "Epoch 3/150 | Train loss: 0.074 | Test loss: 0.073 | Test accuracy: 0.222 | Test top 3 accuracy: 0.379\n",
      "Epoch 4/150 | Train loss: 0.070 | Test loss: 0.071 | Test accuracy: 0.241 | Test top 3 accuracy: 0.398\n",
      "Epoch 5/150 | Train loss: 0.068 | Test loss: 0.069 | Test accuracy: 0.275 | Test top 3 accuracy: 0.405\n",
      "Epoch 6/150 | Train loss: 0.067 | Test loss: 0.068 | Test accuracy: 0.274 | Test top 3 accuracy: 0.421\n",
      "Epoch 7/150 | Train loss: 0.065 | Test loss: 0.067 | Test accuracy: 0.294 | Test top 3 accuracy: 0.403\n",
      "Epoch 8/150 | Train loss: 0.063 | Test loss: 0.066 | Test accuracy: 0.315 | Test top 3 accuracy: 0.441\n",
      "Epoch 9/150 | Train loss: 0.062 | Test loss: 0.065 | Test accuracy: 0.335 | Test top 3 accuracy: 0.470\n",
      "Epoch 10/150 | Train loss: 0.060 | Test loss: 0.064 | Test accuracy: 0.335 | Test top 3 accuracy: 0.463\n",
      "Epoch 11/150 | Train loss: 0.060 | Test loss: 0.063 | Test accuracy: 0.342 | Test top 3 accuracy: 0.462\n",
      "Epoch 12/150 | Train loss: 0.058 | Test loss: 0.063 | Test accuracy: 0.337 | Test top 3 accuracy: 0.472\n",
      "Epoch 13/150 | Train loss: 0.057 | Test loss: 0.062 | Test accuracy: 0.344 | Test top 3 accuracy: 0.479\n",
      "Epoch 14/150 | Train loss: 0.056 | Test loss: 0.061 | Test accuracy: 0.356 | Test top 3 accuracy: 0.479\n",
      "Epoch 15/150 | Train loss: 0.056 | Test loss: 0.061 | Test accuracy: 0.344 | Test top 3 accuracy: 0.482\n",
      "Epoch 16/150 | Train loss: 0.055 | Test loss: 0.060 | Test accuracy: 0.361 | Test top 3 accuracy: 0.484\n",
      "Epoch 17/150 | Train loss: 0.054 | Test loss: 0.060 | Test accuracy: 0.378 | Test top 3 accuracy: 0.496\n",
      "Epoch 18/150 | Train loss: 0.053 | Test loss: 0.059 | Test accuracy: 0.397 | Test top 3 accuracy: 0.497\n",
      "Epoch 19/150 | Train loss: 0.053 | Test loss: 0.059 | Test accuracy: 0.388 | Test top 3 accuracy: 0.518\n",
      "Epoch 20/150 | Train loss: 0.052 | Test loss: 0.058 | Test accuracy: 0.397 | Test top 3 accuracy: 0.511\n",
      "Epoch 21/150 | Train loss: 0.052 | Test loss: 0.058 | Test accuracy: 0.397 | Test top 3 accuracy: 0.523\n",
      "Epoch 22/150 | Train loss: 0.051 | Test loss: 0.058 | Test accuracy: 0.407 | Test top 3 accuracy: 0.530\n",
      "Epoch 23/150 | Train loss: 0.050 | Test loss: 0.057 | Test accuracy: 0.402 | Test top 3 accuracy: 0.540\n",
      "Epoch 24/150 | Train loss: 0.050 | Test loss: 0.057 | Test accuracy: 0.409 | Test top 3 accuracy: 0.538\n",
      "Epoch 25/150 | Train loss: 0.049 | Test loss: 0.057 | Test accuracy: 0.426 | Test top 3 accuracy: 0.550\n",
      "Epoch 26/150 | Train loss: 0.049 | Test loss: 0.056 | Test accuracy: 0.427 | Test top 3 accuracy: 0.544\n",
      "Epoch 27/150 | Train loss: 0.048 | Test loss: 0.056 | Test accuracy: 0.426 | Test top 3 accuracy: 0.547\n",
      "Epoch 28/150 | Train loss: 0.048 | Test loss: 0.056 | Test accuracy: 0.417 | Test top 3 accuracy: 0.554\n",
      "Epoch 29/150 | Train loss: 0.047 | Test loss: 0.055 | Test accuracy: 0.431 | Test top 3 accuracy: 0.557\n",
      "Epoch 30/150 | Train loss: 0.047 | Test loss: 0.055 | Test accuracy: 0.417 | Test top 3 accuracy: 0.571\n",
      "Epoch 31/150 | Train loss: 0.046 | Test loss: 0.055 | Test accuracy: 0.422 | Test top 3 accuracy: 0.550\n",
      "Epoch 32/150 | Train loss: 0.046 | Test loss: 0.055 | Test accuracy: 0.446 | Test top 3 accuracy: 0.576\n",
      "Epoch 33/150 | Train loss: 0.045 | Test loss: 0.054 | Test accuracy: 0.434 | Test top 3 accuracy: 0.581\n",
      "Epoch 34/150 | Train loss: 0.045 | Test loss: 0.054 | Test accuracy: 0.465 | Test top 3 accuracy: 0.562\n",
      "Epoch 35/150 | Train loss: 0.044 | Test loss: 0.054 | Test accuracy: 0.438 | Test top 3 accuracy: 0.578\n",
      "Epoch 36/150 | Train loss: 0.044 | Test loss: 0.054 | Test accuracy: 0.456 | Test top 3 accuracy: 0.574\n",
      "Epoch 37/150 | Train loss: 0.044 | Test loss: 0.054 | Test accuracy: 0.456 | Test top 3 accuracy: 0.581\n",
      "Epoch 38/150 | Train loss: 0.043 | Test loss: 0.053 | Test accuracy: 0.463 | Test top 3 accuracy: 0.576\n",
      "Epoch 39/150 | Train loss: 0.043 | Test loss: 0.053 | Test accuracy: 0.456 | Test top 3 accuracy: 0.591\n",
      "Epoch 40/150 | Train loss: 0.042 | Test loss: 0.053 | Test accuracy: 0.470 | Test top 3 accuracy: 0.595\n",
      "Epoch 41/150 | Train loss: 0.042 | Test loss: 0.053 | Test accuracy: 0.453 | Test top 3 accuracy: 0.597\n",
      "Epoch 42/150 | Train loss: 0.042 | Test loss: 0.053 | Test accuracy: 0.462 | Test top 3 accuracy: 0.597\n",
      "Epoch 43/150 | Train loss: 0.041 | Test loss: 0.053 | Test accuracy: 0.480 | Test top 3 accuracy: 0.607\n",
      "Epoch 44/150 | Train loss: 0.041 | Test loss: 0.052 | Test accuracy: 0.470 | Test top 3 accuracy: 0.602\n",
      "Epoch 45/150 | Train loss: 0.041 | Test loss: 0.052 | Test accuracy: 0.472 | Test top 3 accuracy: 0.602\n",
      "Epoch 46/150 | Train loss: 0.040 | Test loss: 0.052 | Test accuracy: 0.475 | Test top 3 accuracy: 0.602\n",
      "Epoch 47/150 | Train loss: 0.041 | Test loss: 0.052 | Test accuracy: 0.499 | Test top 3 accuracy: 0.602\n",
      "Epoch 48/150 | Train loss: 0.040 | Test loss: 0.052 | Test accuracy: 0.477 | Test top 3 accuracy: 0.615\n",
      "Epoch 49/150 | Train loss: 0.040 | Test loss: 0.052 | Test accuracy: 0.499 | Test top 3 accuracy: 0.602\n",
      "Epoch 50/150 | Train loss: 0.040 | Test loss: 0.052 | Test accuracy: 0.485 | Test top 3 accuracy: 0.607\n",
      "Epoch 51/150 | Train loss: 0.039 | Test loss: 0.052 | Test accuracy: 0.515 | Test top 3 accuracy: 0.619\n",
      "Epoch 52/150 | Train loss: 0.039 | Test loss: 0.052 | Test accuracy: 0.492 | Test top 3 accuracy: 0.609\n",
      "Epoch 53/150 | Train loss: 0.039 | Test loss: 0.051 | Test accuracy: 0.491 | Test top 3 accuracy: 0.614\n",
      "Epoch 54/150 | Train loss: 0.038 | Test loss: 0.051 | Test accuracy: 0.496 | Test top 3 accuracy: 0.612\n",
      "Epoch 55/150 | Train loss: 0.038 | Test loss: 0.051 | Test accuracy: 0.509 | Test top 3 accuracy: 0.624\n",
      "Epoch 56/150 | Train loss: 0.038 | Test loss: 0.051 | Test accuracy: 0.508 | Test top 3 accuracy: 0.621\n",
      "Epoch 57/150 | Train loss: 0.037 | Test loss: 0.051 | Test accuracy: 0.497 | Test top 3 accuracy: 0.626\n",
      "Epoch 58/150 | Train loss: 0.037 | Test loss: 0.051 | Test accuracy: 0.501 | Test top 3 accuracy: 0.624\n",
      "Epoch 59/150 | Train loss: 0.037 | Test loss: 0.051 | Test accuracy: 0.515 | Test top 3 accuracy: 0.624\n",
      "Epoch 60/150 | Train loss: 0.038 | Test loss: 0.051 | Test accuracy: 0.506 | Test top 3 accuracy: 0.621\n",
      "Epoch 61/150 | Train loss: 0.037 | Test loss: 0.051 | Test accuracy: 0.516 | Test top 3 accuracy: 0.626\n",
      "Epoch 62/150 | Train loss: 0.036 | Test loss: 0.051 | Test accuracy: 0.511 | Test top 3 accuracy: 0.626\n",
      "Epoch 63/150 | Train loss: 0.036 | Test loss: 0.051 | Test accuracy: 0.518 | Test top 3 accuracy: 0.626\n",
      "Epoch 64/150 | Train loss: 0.036 | Test loss: 0.051 | Test accuracy: 0.515 | Test top 3 accuracy: 0.631\n",
      "Epoch 65/150 | Train loss: 0.036 | Test loss: 0.051 | Test accuracy: 0.506 | Test top 3 accuracy: 0.626\n",
      "Epoch 66/150 | Train loss: 0.035 | Test loss: 0.051 | Test accuracy: 0.516 | Test top 3 accuracy: 0.632\n",
      "Epoch 67/150 | Train loss: 0.035 | Test loss: 0.050 | Test accuracy: 0.520 | Test top 3 accuracy: 0.631\n",
      "Epoch 68/150 | Train loss: 0.035 | Test loss: 0.050 | Test accuracy: 0.518 | Test top 3 accuracy: 0.639\n",
      "Epoch 69/150 | Train loss: 0.035 | Test loss: 0.050 | Test accuracy: 0.530 | Test top 3 accuracy: 0.641\n",
      "Epoch 70/150 | Train loss: 0.035 | Test loss: 0.050 | Test accuracy: 0.526 | Test top 3 accuracy: 0.634\n",
      "Epoch 71/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.528 | Test top 3 accuracy: 0.641\n",
      "Epoch 72/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.523 | Test top 3 accuracy: 0.641\n",
      "Epoch 73/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.544 | Test top 3 accuracy: 0.641\n",
      "Epoch 74/150 | Train loss: 0.035 | Test loss: 0.050 | Test accuracy: 0.525 | Test top 3 accuracy: 0.651\n",
      "Epoch 75/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.526 | Test top 3 accuracy: 0.650\n",
      "Epoch 76/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.537 | Test top 3 accuracy: 0.644\n",
      "Epoch 77/150 | Train loss: 0.034 | Test loss: 0.050 | Test accuracy: 0.544 | Test top 3 accuracy: 0.648\n",
      "Epoch 78/150 | Train loss: 0.033 | Test loss: 0.050 | Test accuracy: 0.547 | Test top 3 accuracy: 0.651\n",
      "Epoch 79/150 | Train loss: 0.033 | Test loss: 0.050 | Test accuracy: 0.556 | Test top 3 accuracy: 0.651\n",
      "Epoch 80/150 | Train loss: 0.033 | Test loss: 0.050 | Test accuracy: 0.535 | Test top 3 accuracy: 0.646\n",
      "Epoch 81/150 | Train loss: 0.033 | Test loss: 0.050 | Test accuracy: 0.545 | Test top 3 accuracy: 0.644\n",
      "Epoch 82/150 | Train loss: 0.033 | Test loss: 0.050 | Test accuracy: 0.554 | Test top 3 accuracy: 0.648\n",
      "Epoch 83/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.538 | Test top 3 accuracy: 0.660\n",
      "Epoch 84/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.554 | Test top 3 accuracy: 0.660\n",
      "Epoch 85/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.545 | Test top 3 accuracy: 0.653\n",
      "Epoch 86/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.535 | Test top 3 accuracy: 0.662\n",
      "Epoch 87/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.542 | Test top 3 accuracy: 0.651\n",
      "Epoch 88/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.557 | Test top 3 accuracy: 0.660\n",
      "Epoch 89/150 | Train loss: 0.032 | Test loss: 0.050 | Test accuracy: 0.542 | Test top 3 accuracy: 0.658\n",
      "Epoch 90/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.554 | Test top 3 accuracy: 0.660\n",
      "Epoch 91/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.552 | Test top 3 accuracy: 0.663\n",
      "Epoch 92/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.556 | Test top 3 accuracy: 0.665\n",
      "Epoch 93/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.554 | Test top 3 accuracy: 0.655\n",
      "Epoch 94/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.549 | Test top 3 accuracy: 0.655\n",
      "Epoch 95/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.561 | Test top 3 accuracy: 0.658\n",
      "Epoch 96/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.562 | Test top 3 accuracy: 0.667\n",
      "Epoch 97/150 | Train loss: 0.031 | Test loss: 0.050 | Test accuracy: 0.557 | Test top 3 accuracy: 0.655\n",
      "Epoch 98/150 | Train loss: 0.031 | Test loss: 0.051 | Test accuracy: 0.549 | Test top 3 accuracy: 0.662\n",
      "Epoch 99/150 | Train loss: 0.031 | Test loss: 0.051 | Test accuracy: 0.561 | Test top 3 accuracy: 0.667\n",
      "Epoch 100/150 | Train loss: 0.034 | Test loss: 0.051 | Test accuracy: 0.556 | Test top 3 accuracy: 0.667\n",
      "Epoch 101/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.552 | Test top 3 accuracy: 0.668\n",
      "Epoch 102/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.561 | Test top 3 accuracy: 0.667\n",
      "Epoch 103/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.550 | Test top 3 accuracy: 0.663\n",
      "Epoch 104/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.562 | Test top 3 accuracy: 0.656\n",
      "Epoch 105/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.552 | Test top 3 accuracy: 0.665\n",
      "Epoch 106/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.557 | Test top 3 accuracy: 0.668\n",
      "Epoch 107/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.561 | Test top 3 accuracy: 0.679\n",
      "Epoch 108/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.562 | Test top 3 accuracy: 0.658\n",
      "Epoch 109/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.556 | Test top 3 accuracy: 0.677\n",
      "Epoch 110/150 | Train loss: 0.030 | Test loss: 0.051 | Test accuracy: 0.564 | Test top 3 accuracy: 0.668\n",
      "Epoch 111/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.562 | Test top 3 accuracy: 0.668\n",
      "Epoch 112/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.569 | Test top 3 accuracy: 0.663\n",
      "Epoch 113/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.559 | Test top 3 accuracy: 0.667\n",
      "Epoch 114/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.566 | Test top 3 accuracy: 0.680\n",
      "Epoch 115/150 | Train loss: 0.029 | Test loss: 0.051 | Test accuracy: 0.561 | Test top 3 accuracy: 0.670\n",
      "Epoch 116/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.562 | Test top 3 accuracy: 0.670\n",
      "Epoch 117/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.566 | Test top 3 accuracy: 0.672\n",
      "Epoch 118/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.566 | Test top 3 accuracy: 0.672\n",
      "Epoch 119/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.562 | Test top 3 accuracy: 0.674\n",
      "Epoch 120/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.574 | Test top 3 accuracy: 0.667\n",
      "Epoch 121/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.568 | Test top 3 accuracy: 0.670\n",
      "Epoch 122/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.571 | Test top 3 accuracy: 0.670\n",
      "Epoch 123/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.554 | Test top 3 accuracy: 0.680\n",
      "Epoch 124/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.559 | Test top 3 accuracy: 0.677\n",
      "Epoch 125/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.566 | Test top 3 accuracy: 0.672\n",
      "Epoch 126/150 | Train loss: 0.032 | Test loss: 0.052 | Test accuracy: 0.568 | Test top 3 accuracy: 0.674\n",
      "Epoch 127/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.561 | Test top 3 accuracy: 0.663\n",
      "Epoch 128/150 | Train loss: 0.029 | Test loss: 0.052 | Test accuracy: 0.568 | Test top 3 accuracy: 0.670\n",
      "Epoch 129/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.561 | Test top 3 accuracy: 0.670\n",
      "Epoch 130/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.564 | Test top 3 accuracy: 0.668\n",
      "Epoch 131/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.569 | Test top 3 accuracy: 0.667\n",
      "Epoch 132/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.569 | Test top 3 accuracy: 0.668\n",
      "Epoch 133/150 | Train loss: 0.028 | Test loss: 0.052 | Test accuracy: 0.562 | Test top 3 accuracy: 0.670\n",
      "Epoch 134/150 | Train loss: 0.029 | Test loss: 0.053 | Test accuracy: 0.564 | Test top 3 accuracy: 0.668\n",
      "Epoch 135/150 | Train loss: 0.028 | Test loss: 0.053 | Test accuracy: 0.566 | Test top 3 accuracy: 0.670\n",
      "Epoch 136/150 | Train loss: 0.028 | Test loss: 0.053 | Test accuracy: 0.564 | Test top 3 accuracy: 0.674\n"
     ]
    }
   ],
   "source": [
    "X_embedding_train, X_embedding_test, X_go_train, X_go_test, y_train, y_test = train_test_split(X_embedding_t, X_go_t, y_t, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(X_embedding_train, X_go_train, y_train)\n",
    "test_dataset = TensorDataset(X_embedding_test, X_go_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# parameters\n",
    "embed_dim = X_embedding_t.shape[1]   #embedding dimension = 1024\n",
    "go_dim = X_go_t.shape[1]   #go terms dimension = 2000\n",
    "output_dim = y_t.shape[1]   #number of enzymes = 535\n",
    "\n",
    "model = model_embedding(embed_dim, output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# early stopping\n",
    "\n",
    "n_epochs = 150\n",
    "patience = 15\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_without_go(model, train_loader, optimizer, criterion)\n",
    "    acc, acc3, test_loss = test_accuracy_without_go(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train loss: {train_loss:.3f} | Test loss: {test_loss:.3f} | Test accuracy: {acc:.3f} | Test top 3 accuracy: {acc3:.3f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"model_embedding_problem_2_final.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter > patience:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
